import{s as Bs,i as Un,j as As,n as Ns}from"../chunks/scheduler.9b9e513e.js";import{S as Gs,i as Ws,r as Os,u as Us,v as Vs,d as Ys,t as Ks,w as Xs,g as s,s as o,m as Ms,H as Is,h as i,A as l,c as r,n as Ps,F as Ss,k as p,a as n,f as a}from"../chunks/index.781c9930.js";import{P as $s,g as Js,a as Rs}from"../chunks/post_layout.3de33f67.js";function Zs(tt){let c,u=`In this blog post, I will explain how I fine-tuned the “Stable Diffusion” with limited resources to generate synthetic malignant melanoma images. The goal of this project was to demonstrate that we can address class imbalance in medical datasets by generating synthetic data, ultimately enabling the training of more robust deep learning models.
Melanoma
We will talk about:`,m,f,h="<li>Brief information about melanoma, role of synthetic data for augmenting rare medical images</li> <li>Quick overview of medical image synthesis</li> <li>Brief introduction to stable diffusion, what is new in SDXL</li> <li>Fine-tuning techniques for SDXL, comparison between them</li> <li>Preparing medical skin dataset and fine-tuning SD with LoRa</li> <li>Results and discussion</li>",d,F,Vn='<a href="#melanoma-skin-cancer">Melanoma Skin Cancer</a>',lt,N,Yn="Melanoma, a form of skin cancer, arises when melanocytes, the cells responsible for skin pigmentation, undergo uncontrolled growth. This condition is a result of cells in the body losing control over their growth, a hallmark of cancer that can occur in various parts of the body and potentially spread to distant regions.",ot,G,Kn="In many cases, melanoma cells continue to produce melanin, leading to tumors with brown or black pigmentation. However, some melanomas do not synthesize melanin, causing them to exhibit pink, tan, or even white coloration.",rt,W,Xn="Melanomas have the potential to develop anywhere on the skin, though they display a higher likelihood of originating on the trunk (chest and back) in men and on the legs in women. It’s important to note that having darkly pigmented skin does reduce the risk of developing melanoma at the more common sites. However, it’s crucial to emphasize that <strong>anyone</strong>, including individuals with darker skin, can indeed develop melanoma on the palms of their hands, the soles of their feet, or beneath their nails.",pt,y,$n='<a href="#melanoma-in-people-of-color">Melanoma in People of Color</a>',ct,O,Jn="It’s worth highlighting that melanoma diagnoses in Black individuals tend to occur at later stages, increasing the risk of severe cases. Darker skin contains a higher number of melanocytes, the pigment-producing cells that offer some level of protection against sun-induced skin damage, including the kind that leads to skin cancer. Darker skin is less prone to sunburn, which has sometimes led to the misconception that Black individuals are immune to skin cancer.",ht,U,Zn="This misconception can result in a lack of skin self-examinations for cancer, delayed medical consultations for skin changes. Moreover, even artificial intelligence systems designed to assist in early cancer detection may fail to adequately identify melanoma on darker skin if they lack a diverse dataset, including representative samples of individuals with darker skin tones.",dt,V,Qn='To address this disparity, the use of synthetic data can play a valuable role in expanding the available data for melanoma detection in Black populations, improving diagnostic accuracy, and ultimately ensuring that all individuals, regardless of their skin tone, receive the appropriate care and attention in the fight against melanoma<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>.',ft,v,ea='<a href="#key-indicators-of-melanoma">Key Indicators of Melanoma</a>',Ft,Y,ta='Finding melanoma at an early stage is crucial; early detection can vastly increase your chances for cure. The first five letters of the alphabet are a guide to help you recognize the warning signs of melanoma. <strong>A is for Asymmetry</strong>, <strong>B is for Border</strong>, <strong>C is for Color</strong>, <strong>D is for Diameter or Dark</strong>, and <strong>E is for Evolving</strong>. More information can be found <a href="https://www.skincancer.org/skin-cancer-information/melanoma/melanoma-warning-signs-and-images/" rel="nofollow noopener noreferrer external" target="_blank">here</a>.',mt,K,na='<img src="img/melanoma_detection.webp" alt="key indicators" width="430"/>',ut,g,aa='<a href="#medical-image-synthesis">Medical Image Synthesis</a>',yt,X,sa='In the history of medical image synthesis, data scarcity in healthcare has driven the usage of generative models like Generative Adversarial Networks (GANs)<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup> and Variational Autoencoders (VAEs). GANs use a competitive game between a generator and discriminator to create increasingly realistic synthetic data, while VAEs learn a probabilistic mapping to balance data fidelity and diversity in their generated samples. You can find lots of resources on internet about how they work.',vt,$,ia="GANs are powerful but troubled by learning instability, convergence issues, and mode collapse. On the other hand, VAEs excel in output diversity and avoid mode collapse, but often produce blurry and hazy images. Recent advances, such as denoising diffusion probabilistic models (DDPMs) and latent DDPMs, have outperformed other techniques in generating natural images. Thus, their performance in medical image synthesis is a current research topic.",gt,J,la="So, we will be exploring the capabilities of diffusion models, with a special focus on a particular model called the “Stable Diffusion”. This model has gained acclaim for its proficiency in generating highly realistic images.",Dt,D,oa='<a href="#stable-diffusion">Stable Diffusion</a>',wt,Z,ra="Basically, Stable Diffusion is a text-to-image model that takes a prompt for input and then outputs an image that corresponds to that. First, they take input data and progressively introduce Gaussian noise in a controlled manner, creating a sort of “noisy” version of the data. This process is like a delicate recipe for noise creation. Then, the real power of these models comes into play. They are trained to reverse this diffusion process, effectively removing the noise and returning clean data from the initially noisy samples.",xt,Q,pa='<img src="img/sd_arch.webp" alt="sd" width="480"/>',bt,ee,ca='<li><a href="https://jalammar.github.io/illustrated-stable-diffusion/" rel="nofollow noopener noreferrer external" target="_blank">Jay Alammar</a> explains diffusion models and what is happening behind the stable diffusion with illustrations.</li> <li>You can check <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="nofollow noopener noreferrer external" target="_blank">Lilian Weng</a> if you want to learn math behind SD.</li> <li>What you are looking for is more practical knowledge, then SD <a href="https://www.reddit.com/r/StableDiffusion/" rel="nofollow noopener noreferrer external" target="_blank">subreddit</a> is way to go. You can find practical tips and more resources to get started.</li>',Ct,te,ha="SDXL is the latest version of the stable diffusion. To highlight the differences from the previous version of SD, here’s a concise list:",Et,ne,da="<li>Larger UNet backbone (three times larger)</li> <li>Increased model parameters with more attention blocks</li> <li>Introduction of a second text encoder</li> <li>Training on multiple aspect ratios</li> <li>Addition of a refinement model for improving visual fidelity</li>",_t,ae,fa="So, it’s undoubtedly more powerful and capable of generating fantastic results with even shorter prompts.",kt,w,Fa='<a href="#fine-tuning-methods-for-sd">Fine-tuning Methods for SD</a>',Lt,se,ma="Training large foundation models like Stable Diffusion from scratch is a formidable task due to their massive computational requirements, extended training times, and complex data needs. So, training these massive models from scratch is a pretty daunting task.",Tt,ie,ua="That’s why many folks in the field are going for a more practical option. Instead of building everything from the ground up, they’re fine-tuning existing pretrained models. It’s a smart move because it saves time and resources and still gets you impressive results. However, there is not a single way to fine tune SD models.",Ht,x,ya='<a href="#traditional-naive-fine-tuning-nft"><strong>Traditional Naive Fine-tuning (NFT)</strong></a>',At,le,va="This is classic fine-tuning for neural networks. Naive Fine-tuning neural networks means adjusting a pre-trained model for a new task, capitalizing on its prior knowledge while making it more suitable for the specific problem at hand. It alters the original model weights. New model can adapt highly specific use cases without prompt engineering.",Mt,oe,ga="However this process requires very large dataset, in terms of SD I recommend minimum dataset size of 1000, and you have to create complex captions for that dataset. Compared to other fine-tuning methods, this technique requires more training steps.",It,b,Da='<a href="#dreambooth">Dreambooth</a>',Pt,re,wa='Dreambooth<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup> approach is for “personalization” of text-to-image diffusion models. By providing a few reference images of a subject, the model fine-tunes itself to connect a unique identifier to that specific subject. Then, when you give it a text prompt, it can generate a range of lifelike images, each one capturing the essence of the subject, all while being versatile and creative in different contexts. More info can be found <a href="https://huggingface.co/blog/dreambooth" rel="nofollow noopener noreferrer external" target="_blank">here</a>.',St,pe,xa="For Dreambooth training you need to have a dataset of thing you want to train on also you need a regularization dataset. Regularization is used for making the model more robust for over-fitting. However, I have seen some folks that are not using it and still got great results. So, it is a bit trial and error with your dataset, which I think the most important aspect of training, and hyperparameter tuning such as finding a sweet spot for learning rate.",Rt,C,ba='<a href="#lora">LoRa</a>',zt,ce,Ca='LoRA<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup> (Low-Rank Adaptation) is a novel technique that originally invented for fine-tuning large-language models. It retains the pre-trained model’s core while incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This strategic move significantly reduces the number of parameters requiring fine-tuning and, notably, alleviates the GPU memory burden.',jt,he,Ea="LoRA focuses on the cross-attention layers on the model, where textual and visual elements intersect. Researchers have found that by fine-tuning this specific aspect of the model, LoRA can achieve remarkable results, making it a compelling choice for optimizing large language models across a range of real-world tasks and applications. In order to train a LoRa, you need images of the subject and corresponding captions for them.",qt,E,_a='<a href="#comparison">Comparison</a>',Bt,de,ka="There are also other methods such as textual-inversion and hypernetworks. However, I think their performance are not suitable for medical image synthesizing. So, I will not mention them in this post. Also some people combine some methods together, for example, Dreambooth with textual inversion or LoRa training with Dreambooth style. I haven’t tried those myself so I don’t have any recommendations about that.",Nt,_,La='<a href="#computational-perspective">Computational Perspective</a>',Gt,fe,Ta="If we compare these 3 methods in terms of computational intensity: Traditional Fine-tuning > Dreambooth > LoRa. You’ll require an A100 or a graphics card with at least 24GB of VRAM for the traditional approach. LoRa and Dreambooth, on the other hand, can be trained with 16GB of VRAM. With further optimization, it’s possible to train these methods on machines with 8-12GB of VRAM.",Wt,k,Ha='<a href="#size-on-disk">Size on Disk</a>',Ot,Fe,Aa="In terms of size, LoRa is the smallest among them. It is only 200 - 600MB in size generally. Other methods alters the original model and saves the whole weights. So they are bigger and take up few gigabytes on disk. This factor becomes important when you want to train different subjects or iterate over same subject with different parameters.",Ut,L,Ma='<a href="#why-i-chose-lora">Why I Chose LoRa</a>',Vt,me,Ia="I chose to use LoRa for this task for some compelling reasons. First, LoRa is lightweight in terms of system requirements, and you can train it even with a free-tier Google Colab using a T4 GPU, making it accessible. It’s also speedy to train because it doesn’t alter the model’s original weights and focuses on a smaller set of parameters. Moreover, LoRa is much smaller in size. It is significantly smaller than traditional checkpoints. Given these advantages, I decided to test the performance of SDXL combined with LoRa in the field of medical imaging.",Yt,T,Pa='<a href="#fine-tuning-based-on-your-goal">Fine-tuning Based on Your Goal</a>',Kt,ue,Sa="Ultimately, regardless of the chosen method, the specific aim you pursue determines the duration of training, the requisite hyperparameters, and the size of your dataset. The nature of what you aim to teach the model significantly influences these parameters. For instance, teaching a simple character might demand only 10-100 photos, while introducing intricate details in inference such as wings, requires more images and specific hyperparameters to match. Similarly, teaching a distinct style, like a self-created style such as ”<em>behlulism</em>,” necessitates a more extensive dataset.",Xt,ye,Ra="In the case of medical images, the need for comprehensive data becomes apparent—gathering all available data becomes crucial for effective training in this domain. Because the base model knows nothing about it and you have to teach that concept. You might wonder, “Why fine-tune the model when it knows nothing about the subject I want to teach?” Well, in this approach, the magic lies in the pretrained weights. They equip the model not just with knowledge of your subject but also with a powerful understanding of what distinctly <em>isn’t</em> your subject.",$t,H,za='<a href="#data-preparation">Data Preparation</a>',Jt,ve,ja='The dataset was generated by the International Skin Imaging Collaboration ISIC<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup> and they hosted a competition on Kaggle. In this competition, there were very few melanoma images compared to a ton of healthy ones – just 584 melanoma images and 32,542 healthy ones. To tackle this imbalance, the winning team had to bring in last year’s competition dataset because there simply weren’t enough melanoma images available<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>.',Zt,ge,qa='<img src="img/melanoma_samples.webp" alt="samples from dataset" width=""/>',Qt,De,Ba="I wanted to use this dataset for fine-tuning SD with LoRa. In order to do that, I need to extract melanoma images from the dataset and also I have to create captions for that images. In CSV file, there is information about the name of the image, gender of the patient, age, and if the mole is malignant or not. I used that information for separating malignant images and captioning them.",en,we,Na="To start, I created a simple script that scans the <em>‘train.csv’</em> file to identify the malignant melanoma images and extracts them. After separating malignant images, I created another simple script for captioning:",tn,nt,js=`<pre class="shiki material-default" style="background-color: #263238; color: #EEFFFF" python="true"><div class="language-id">python</div><div class='code-container'><code><div class='line'><span style="color: #C792EA">def</span><span style="color: #EEFFFF"> </span><span style="color: #82AAFF">create_caption</span><span style="color: #89DDFF">(</span><span style="color: #EEFFFF">row</span><span style="color: #89DDFF">):</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #89DDFF">"""</span><span style="color: #546E7A">Create a caption for an image.</span><span style="color: #89DDFF">"""</span></div><div class='line'><span style="color: #EEFFFF">    sex </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> row</span><span style="color: #89DDFF">[</span><span style="color: #F78C6C">2</span><span style="color: #89DDFF">]</span></div><div class='line'><span style="color: #EEFFFF">    anatom_site </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> row</span><span style="color: #89DDFF">[</span><span style="color: #F78C6C">4</span><span style="color: #89DDFF">]</span></div><div class='line'><span style="color: #EEFFFF">    age </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> row</span><span style="color: #89DDFF">[</span><span style="color: #F78C6C">3</span><span style="color: #89DDFF">]</span></div><div class='line'><span style="color: #EEFFFF">    benign_malignant </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> row</span><span style="color: #89DDFF">[</span><span style="color: #F78C6C">6</span><span style="color: #89DDFF">]</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #89DDFF">if</span><span style="color: #EEFFFF"> benign_malignant</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">lower</span><span style="color: #89DDFF">()</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">==</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">'</span><span style="color: #C3E88D">malignant</span><span style="color: #89DDFF">'</span><span style="color: #89DDFF">:</span></div><div class='line'><span style="color: #EEFFFF">        sex_caption </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> sex </span><span style="color: #89DDFF">or</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">""</span></div><div class='line'><span style="color: #EEFFFF">        anatom_site_caption </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> </span><span style="color: #C792EA">f</span><span style="color: #C3E88D">"on </span><span style="color: #EEFFFF">&#123;anatom_site&#125;</span><span style="color: #C3E88D">"</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">if</span><span style="color: #EEFFFF"> anatom_site </span><span style="color: #89DDFF">else</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">""</span></div><div class='line'><span style="color: #EEFFFF">        age_caption </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> </span><span style="color: #C792EA">f</span><span style="color: #C3E88D">", at age </span><span style="color: #EEFFFF">&#123;</span><span style="color: #FFCB6B">int</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">age</span><span style="color: #89DDFF">)</span><span style="color: #EEFFFF">&#125;</span><span style="color: #C3E88D">"</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">if</span><span style="color: #EEFFFF"> age </span><span style="color: #89DDFF">else</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">""</span></div><div class='line'><span style="color: #EEFFFF">        caption </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> </span><span style="color: #C792EA">f</span><span style="color: #C3E88D">"photo of </span><span style="color: #EEFFFF">&#123;sex_caption&#125;</span><span style="color: #C3E88D"> skin, melanoma </span><span style="color: #EEFFFF">&#123;anatom_site_caption&#125;&#123;age_caption&#125;</span><span style="color: #C3E88D">"</span></div><div class='line'><span style="color: #EEFFFF">        </span><span style="color: #89DDFF">return</span><span style="color: #EEFFFF"> caption</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #89DDFF">else</span><span style="color: #89DDFF">:</span></div><div class='line'><span style="color: #EEFFFF">        </span><span style="color: #89DDFF">return</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">None</span></div></code></div></pre>`,at,xe,Ga="I utilized the data within the CSV file to generate diverse captions based on gender, age, and the anatomical site. Because I wanted more variation in captions, I created a script that analyzes if there is hair in the image.",nn,st,qs=`<pre class="shiki material-default" style="background-color: #263238; color: #EEFFFF" python="true"><div class="language-id">python</div><div class='code-container'><code><div class='line'><span style="color: #C792EA">def</span><span style="color: #EEFFFF"> </span><span style="color: #82AAFF">process_image</span><span style="color: #89DDFF">(</span><span style="color: #EEFFFF">image_path</span><span style="color: #89DDFF">):</span></div><div class='line'><span style="color: #EEFFFF">	</span><span style="color: #89DDFF">"""</span><span style="color: #546E7A">Analyze the images if they are hairy or not.</span><span style="color: #89DDFF">"""</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># Load the image</span></div><div class='line'><span style="color: #EEFFFF">    image </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">imread</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">image_path</span><span style="color: #89DDFF">)</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># Convert the image to grayscale</span></div><div class='line'><span style="color: #EEFFFF">    gray_image </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">cvtColor</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">image</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #F07178">COLOR_BGR2GRAY</span><span style="color: #89DDFF">)</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># Apply Gaussian blur to reduce noise and improve contour detection</span></div><div class='line'><span style="color: #EEFFFF">    blurred_image </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">GaussianBlur</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">gray_image</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> </span><span style="color: #89DDFF">(</span><span style="color: #F78C6C">5</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> </span><span style="color: #F78C6C">5</span><span style="color: #89DDFF">),</span><span style="color: #82AAFF"> </span><span style="color: #F78C6C">0</span><span style="color: #89DDFF">)</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># Use Canny edge detection to find edges in the image</span></div><div class='line'><span style="color: #EEFFFF">    edges </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">Canny</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">blurred_image</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> </span><span style="color: #F78C6C">50</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> </span><span style="color: #F78C6C">250</span><span style="color: #89DDFF">)</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># Find contours in the edge image</span></div><div class='line'><span style="color: #EEFFFF">    contours</span><span style="color: #89DDFF">,</span><span style="color: #EEFFFF"> _ </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">findContours</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">edges</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #F07178">RETR_EXTERNAL</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #F07178">CHAIN_APPROX_SIMPLE</span><span style="color: #89DDFF">)</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># Define a minimum contour length threshold to filter out small artifacts</span></div><div class='line'><span style="color: #EEFFFF">    min_contour_length </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> </span><span style="color: #F78C6C">100</span><span style="color: #EEFFFF">  </span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># Filter out small contours (artifacts)</span></div><div class='line'><span style="color: #EEFFFF">    filtered_contours </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">[</span><span style="color: #EEFFFF">contour </span><span style="color: #89DDFF">for</span><span style="color: #EEFFFF"> contour </span><span style="color: #89DDFF">in</span><span style="color: #EEFFFF"> contours </span><span style="color: #89DDFF">if</span><span style="color: #EEFFFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">arcLength</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">contour</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> </span><span style="color: #89DDFF">False)</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">&gt;=</span><span style="color: #EEFFFF"> min_contour_length</span><span style="color: #89DDFF">]</span></div><div class='line'><span style="color: #EEFFFF">    total_perimeter </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> </span><span style="color: #F78C6C">0</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #89DDFF">for</span><span style="color: #EEFFFF"> contour </span><span style="color: #89DDFF">in</span><span style="color: #EEFFFF"> filtered_contours</span><span style="color: #89DDFF">:</span></div><div class='line'><span style="color: #EEFFFF">        perimeter </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> cv2</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">arcLength</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">contour</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> </span><span style="color: #89DDFF">False)</span></div><div class='line'><span style="color: #EEFFFF">        total_perimeter </span><span style="color: #89DDFF">+=</span><span style="color: #EEFFFF"> perimeter</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #82AAFF">print</span><span style="color: #89DDFF">(</span><span style="color: #C792EA">f</span><span style="color: #C3E88D">'Total Perimeter: </span><span style="color: #82AAFF">&#123;total_perimeter&#125;</span><span style="color: #C3E88D">'</span><span style="color: #89DDFF">)</span></div><div class='line'><span style="color: #EEFFFF">    name_of_caption </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> image_path</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">split</span><span style="color: #89DDFF">(</span><span style="color: #89DDFF">"</span><span style="color: #C3E88D">.</span><span style="color: #89DDFF">"</span><span style="color: #89DDFF">)</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #546E7A"># If the total perimeter is greater than 4000, add ', hairy' to the caption</span></div><div class='line'><span style="color: #EEFFFF">    </span><span style="color: #89DDFF">if</span><span style="color: #EEFFFF"> total_perimeter </span><span style="color: #89DDFF">&gt;</span><span style="color: #EEFFFF"> </span><span style="color: #F78C6C">4000</span><span style="color: #89DDFF">:</span></div><div class='line'><span style="color: #EEFFFF">        </span><span style="color: #546E7A"># Add ', hairy' to the caption file associated with this image</span></div><div class='line'><span style="color: #EEFFFF">        caption_filename </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> name_of_caption</span><span style="color: #89DDFF">[</span><span style="color: #F78C6C">0</span><span style="color: #89DDFF">]</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">+</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">'</span><span style="color: #C3E88D">.caption</span><span style="color: #89DDFF">'</span></div><div class='line'><span style="color: #EEFFFF">        </span><span style="color: #89DDFF">with</span><span style="color: #EEFFFF"> </span><span style="color: #82AAFF">open</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">caption_filename</span><span style="color: #89DDFF">,</span><span style="color: #82AAFF"> </span><span style="color: #89DDFF">'</span><span style="color: #C3E88D">a</span><span style="color: #89DDFF">'</span><span style="color: #89DDFF">)</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">as</span><span style="color: #EEFFFF"> file</span><span style="color: #89DDFF">:</span></div><div class='line'><span style="color: #EEFFFF">            text_to_add </span><span style="color: #89DDFF">=</span><span style="color: #EEFFFF"> </span><span style="color: #89DDFF">'</span><span style="color: #C3E88D">, hairy</span><span style="color: #89DDFF">'</span></div><div class='line'><span style="color: #EEFFFF">            file</span><span style="color: #89DDFF">.</span><span style="color: #82AAFF">write</span><span style="color: #89DDFF">(</span><span style="color: #82AAFF">text_to_add</span><span style="color: #89DDFF">)</span></div></code></div></pre>`,it,be,Wa="I think the code is quite self explanatory. Essentially, it identifies edge counters in the images, summing them up to compare against a predefined threshold to determine if there’s sufficient hair in the image. When hair is detected, it appends a “hairy” caption to the existing files. It’s worth noting that the script is experimental and not without its imperfections, mainly because of the dynamic nature of medical images. That’s why I decided to keep it separate from the original captioning script.",an,Ce,Oa=`Finally, the created captions will be in the format of:
photo of a (sex) skin, me1anoma on (anatom_site), at age (age_approx), (hairy).`,sn,A,Ua='<a href="#fine-tuning-sd-with-lora">Fine-tuning SD with LoRa</a>',ln,Ee,Va='For fine-tuning purposes, I employed the scripts available from <a href="https://github.com/kohya-ss/sd-scripts" rel="nofollow noopener noreferrer external" target="_blank">kohya_ss</a>. Alternatively, you can also use the <a href="https://github.com/huggingface/diffusers/" rel="nofollow noopener noreferrer external" target="_blank">diffusers</a> library from Hugging Face. The kohya_ss repository offers various versions and experimental models, making it worth exploring.',on,M,Ya='<a href="#base-model">Base Model</a>',rn,_e,Ka='To begin with, LoRa necessitates a base model for training. There are two approaches regarding the choice of base model. Some people advocate consistently using standard checkpoints (pretrained model weights) like SD 1.5 or SDXL 1.0. Others suggest selecting a checkpoint aligned with your specific objectives. For example, if your aim is to generate highly realistic images of the character you’re training on, a specialized checkpoint with a focus on realism, such as the one provided <a href="https://civitai.com/models/152525/realism-engine-sdxl" rel="nofollow noopener noreferrer external" target="_blank">here</a>, should be your choice. On the other hand, if you want to create anime-like images, a base model specialized for anime generation is preferable.',pn,ke,Xa="Both approaches make sense, and your choice really depends on what you’re trying to achieve. If you want to transfer yourself or a real world person into the world of SD, then those checkpoints with a focus on realism are great because they understand how real people look and give you a head start while fine-tuning. However, if you want to capture a specific style or something unique, like medical images, it’s better to go with base models like SDXL 1.0 or SD 1.5. They’re less biased towards any specific outcome.",cn,I,$a='<a href="#note-on-beauty-standards-of-models">Note on Beauty Standards of Models</a>',hn,Le,Ja='I want to highlight that these models often aim to produce images that align with what are considered ‘stereotypical beauty standards’, focusing on achieving ‘perfect’ body and facial features. During my experimentation with these models and fine-tuning processes, I wanted to transfer my girlfriend to the world of SD. She has a beautiful feature, a charming nasal hump on her nose. However, when generating images, we often struggled to retain this nasal hump in the resulting images because the model tends to prioritize conforming to these conventional beauty norms. In response to this, there are even checkpoints available that specifically aim to depict more natural <a href="https://civitai.com/models/98755/humans" rel="nofollow noopener noreferrer external" target="_blank">humans</a>.',dn,Te,Za='<img src="img/loratestresult.webp" alt="lora results" width=""/>',fn,He,Qa="<em>One of the best resuls we achieved; left real right synthetic</em>",Fn,Ae,es='I’d love to delve deeper into this topic and explore the biases in these models, for example race and gender discrimination is hugely biased in these models, but that would be a whole new blog subject. In the meantime, you can check out <a href="https://www.bloomberg.com/graphics/2023-generative-ai-bias/" rel="nofollow noopener noreferrer external" target="_blank">this article</a>.',mn,P,ts='<a href="#training-configs">Training Configs</a>',un,S,ns='<a href="#lora-configs">LoRa Configs</a>',yn,Me,as="LoRa comes in various types, including LoRA-C3Lier, LoHa, and DyLoRA. Each type alters the kernel size of the 2D convolution layer and makes adjustments to the architecture. You can find more information on the internet. For this project, I’ll be using Kohya’s standard LoRa, which has been renamed as ‘LoRA-LierLa’. Two specific configurations need to be set for this model:",vn,Ie,ss="<strong>network_dim:</strong> The network dimensions, or rank, indicates how many parameters of the Unet/TE to train. The higher the value, the greater the “expressive power” of the model, but at the expense of larger file size. Most of the time 64 is suffice if you want to train something generic. However, when dealing with more specialized subjects 128 is a better choice. In the case of medical images, I choose my rank as 128.",gn,Pe,is=`<strong>network_alpha:</strong> Network alpha is like a learning control knob – it’s there to put the brakes on or, in other words, dampen the learning process. Essentially, it’s used to adjust the model’s weights during saving to prevent any rounding errors that might mess up some of the weight values.
Setting alpha to 0 or making it equal to network dimensions (net dim) doesn’t apply any dampening. However, when set to 1, it introduces significant dampening, requiring more training steps or a higher learning rate. A comparison of different scaling options is provided below:`,Dn,Se,ls="Alpha 0 = Alpha 128 = 128/128 = x1",wn,Re,os="Alpha 1 = 1/128 = x0.0078125",xn,ze,rs="Alpha 64 = 64/128 = x0.5",bn,je,ps="Alpha 128 = 128/128 = x1",Cn,qe,cs="Alpha 256 = 256/128 = x2",En,Be,hs="When in doubt, you can set it equal to the network_dim.",_n,R,ds='<a href="#optimizer-configs">Optimizer Configs</a>',kn,Ne,fs="In my opinion, AdamW stands out as one of the best optimizers available for this task, provided your system can handle the computational cost. If you’re working with budget constraints, an alternative optimizer to consider is Adafactor. It uses less memory by only storing partial information from previous gradients. Another solid choice is the 8-bit version of AdamW, known as AdamW8bit.",Ln,Ge,Fs="As a note, AdamW8bit performed better than Adafactor in my case. However, I encourage you to experiment and explore options to see what works best for your particular project.",Tn,We,ms="As learning rate, I think it is also highly dependent on what you are trying to achieve. I have seen that the value of it can range anywhere from 4e-7, which is the standard learning rate for SDXL, to 8e-5. It is highly dependent on the dataset. If you are trying to teach a usual concept like a person, it is safe to increase it. However, if you are trying to teach a more unique concept, like medical images, lowering it down will be more beneficial. And don’t forget that as you decrease the learning rate, you have to train the model longer.",Hn,Oe,us="I use a constant learning rate scheduler with a warm-up period of 100 steps. While some people recommend the cosine learning rate scheduler for better performance, I believe it can be a bit of a trial-and-error process to find the right fit for your dataset.",An,z,ys='<a href="#general-config">General Config</a>',Mn,Ue,vs="For my other configurations, I typically set the number of repeats to 1 while aiming to maximize the number of epochs. There’s a parameter that determines how frequently the training script saves snapshots of the current model status. This periodic saving also triggers an inference run on the model with a predefined input prompt for testing. I monitor the generated images closely to determine whether LoRa is ready for the task. If it starts producing images with noticeable artifacts, it’s usually a sign of overfitting.",In,j,gs='<a href="#results-and-discussion">Results and Discussion</a>',Pn,Ve,Ds="I trained several LoRa models. I used different learning rates, optimizers, and schedulers that I talked about through this blog post. Finally, I achieved some satisfactory results.",Sn,Ye,ws='<img src="img/melanomasynthetic.webp" alt="synthetic melanoma" width="480"/>',Rn,Ke,xs="<li>Sampler: Eular a, DPM++ 2M/3M Karras</li>",zn,Xe,bs="<li>20-30 Sampling steps</li> <li>7.5-9 CFG Scale</li>",jn,$e,Cs='When I applied to the ABCDE rule that we talked about in <a href="#medical-image-synthesis">medical images section</a> to the synthesized images, I have seen that the most of the generations passed the test. However, when I looked at carefully for some images, I have seen that some of them are little blurry like the lower right corner. I think the reason for that could be the dataset.',qn,Je,Es="Following that, I decided to present a set of synthetic images to a group of professionals I am acquainted with. I explained the process and asked them to distinguish the synthetic images from the real ones. After examining the images, we engaged in a discussion about their observations and methodology. The professionals mentioned that they often perceived an image as synthetic when it exhibited fewer details. However, there was a complication as some real images in the dataset were quite simple and were mistakenly labeled as synthetic. Additionally, they encountered challenges when some complex synthetic images, which included elements like hair or red dots, were incorrectly identified as real by the examiners.",Bn,Ze,_s="The results are promising, though not perfect at this stage. To further this project, collaboration with pathologists and dermatologists is essential. I firmly believe that the application of diffusion models in generating synthetic medical images holds great potential in this field.",Nn,Qe,ks="Remarkably, I find it quite astonishing that by simply fine-tuning with LoRa, we are able to generate medical images. This process has shown a realm of new exciting possibilities for me. It’s especially significant due to the remarkably low costs involved, both in terms of system resources and time. Additionally, the compact model weights make it convenient to store and re-train various LoRas, offering substantial versatility for various applications.",Gn,q,Ls='<a href="#references-and-notes">References and Notes</a>',Wn,et,Ts='All the code I wrote can be found in this <a href="https://github.com/lioga/melanoma-stable-diffusion" rel="nofollow noopener noreferrer external" target="_blank">repo</a>.',On,B,Hs='<hr/> <ol><li id="fn-1">In order to achieve that task at least some amount of data is required to train generative AI model.<a href="#fnref-1" class="footnote-backref">↩</a></li> <li id="fn-2">Chen, R.J., Lu, M.Y., Chen, T.Y. <em>et al.</em> Synthetic data in machine learning for medicine and healthcare. <em>Nat Biomed Eng</em> <strong>5</strong>, 493–497 (2021). <a href="https://doi.org/10.1038/s41551-021-00751-8" rel="nofollow noopener noreferrer external" target="_blank">https://doi.org/10.1038/s41551-021-00751-8</a><a href="#fnref-2" class="footnote-backref">↩</a></li> <li id="fn-3"><a href="https://arxiv.org/abs/2208.12242" rel="nofollow noopener noreferrer external" target="_blank">https://arxiv.org/abs/2208.12242</a><a href="#fnref-3" class="footnote-backref">↩</a></li> <li id="fn-4"><a href="https://arxiv.org/abs/2106.09685" rel="nofollow noopener noreferrer external" target="_blank">https://arxiv.org/abs/2106.09685</a><a href="#fnref-4" class="footnote-backref">↩</a></li> <li id="fn-5">Rotemberg, V., Kurtansky, N., Betz-Stablein, B., Caffery, L., Chousakos, E., Codella, N., Combalia, M., Dusza, S., Guitera, P., Gutman, D., Halpern, A., Helba, B., Kittler, H., Kose, K., Langer, S., Lioprys, K., Malvehy, J., Musthaq, S., Nanda, J., Reiter, O., Shih, G., Stratigos, A., Tschandl, P., Weber, J. &amp; Soyer, P. A patient-centric dataset of images and metadata for identifying melanomas using clinical context. Sci Data 8, 34 (2021). <a href="https://doi.org/10.1038/s41597-021-00815-z" rel="nofollow noopener noreferrer external" target="_blank">https://doi.org/10.1038/s41597-021-00815-z</a><a href="#fnref-5" class="footnote-backref">↩</a></li> <li id="fn-6"><a href="https://arxiv.org/abs/2010.05351" rel="nofollow noopener noreferrer external" target="_blank">https://arxiv.org/abs/2010.05351</a><a href="#fnref-6" class="footnote-backref">↩</a></li></ol>';return{c(){c=s("p"),c.textContent=u,m=o(),f=s("ul"),f.innerHTML=h,d=o(),F=s("h1"),F.innerHTML=Vn,lt=o(),N=s("p"),N.textContent=Yn,ot=o(),G=s("p"),G.textContent=Kn,rt=o(),W=s("p"),W.innerHTML=Xn,pt=o(),y=s("h2"),y.innerHTML=$n,ct=o(),O=s("p"),O.textContent=Jn,ht=o(),U=s("p"),U.textContent=Zn,dt=o(),V=s("p"),V.innerHTML=Qn,ft=o(),v=s("h2"),v.innerHTML=ea,Ft=o(),Y=s("p"),Y.innerHTML=ta,mt=o(),K=s("center"),K.innerHTML=na,ut=o(),g=s("h1"),g.innerHTML=aa,yt=o(),X=s("p"),X.innerHTML=sa,vt=o(),$=s("p"),$.textContent=ia,gt=o(),J=s("p"),J.textContent=la,Dt=o(),D=s("h1"),D.innerHTML=oa,wt=o(),Z=s("p"),Z.textContent=ra,xt=o(),Q=s("center"),Q.innerHTML=pa,bt=Ms(`
I don't want to deep dive into how Stable Diffusion works but there are great resources that I can suggest for you:
`),ee=s("ul"),ee.innerHTML=ca,Ct=o(),te=s("p"),te.textContent=ha,Et=o(),ne=s("ul"),ne.innerHTML=da,_t=o(),ae=s("p"),ae.textContent=fa,kt=o(),w=s("h1"),w.innerHTML=Fa,Lt=o(),se=s("p"),se.textContent=ma,Tt=o(),ie=s("p"),ie.textContent=ua,Ht=o(),x=s("h4"),x.innerHTML=ya,At=o(),le=s("p"),le.textContent=va,Mt=o(),oe=s("p"),oe.textContent=ga,It=o(),b=s("h4"),b.innerHTML=Da,Pt=o(),re=s("p"),re.innerHTML=wa,St=o(),pe=s("p"),pe.textContent=xa,Rt=o(),C=s("h4"),C.innerHTML=ba,zt=o(),ce=s("p"),ce.innerHTML=Ca,jt=o(),he=s("p"),he.textContent=Ea,qt=o(),E=s("h2"),E.innerHTML=_a,Bt=o(),de=s("p"),de.textContent=ka,Nt=o(),_=s("h4"),_.innerHTML=La,Gt=o(),fe=s("p"),fe.textContent=Ta,Wt=o(),k=s("h4"),k.innerHTML=Ha,Ot=o(),Fe=s("p"),Fe.textContent=Aa,Ut=o(),L=s("h4"),L.innerHTML=Ma,Vt=o(),me=s("p"),me.textContent=Ia,Yt=o(),T=s("h4"),T.innerHTML=Pa,Kt=o(),ue=s("p"),ue.innerHTML=Sa,Xt=o(),ye=s("p"),ye.innerHTML=Ra,$t=o(),H=s("h1"),H.innerHTML=za,Jt=o(),ve=s("p"),ve.innerHTML=ja,Zt=o(),ge=s("center"),ge.innerHTML=qa,Qt=o(),De=s("p"),De.textContent=Ba,en=o(),we=s("p"),we.innerHTML=Na,tn=o(),nt=new Is(!1),at=o(),xe=s("p"),xe.textContent=Ga,nn=o(),st=new Is(!1),it=o(),be=s("p"),be.textContent=Wa,an=o(),Ce=s("p"),Ce.textContent=Oa,sn=o(),A=s("h1"),A.innerHTML=Ua,ln=o(),Ee=s("p"),Ee.innerHTML=Va,on=o(),M=s("h2"),M.innerHTML=Ya,rn=o(),_e=s("p"),_e.innerHTML=Ka,pn=o(),ke=s("p"),ke.textContent=Xa,cn=o(),I=s("h2"),I.innerHTML=$a,hn=o(),Le=s("p"),Le.innerHTML=Ja,dn=o(),Te=s("center"),Te.innerHTML=Za,fn=o(),He=s("p"),He.innerHTML=Qa,Fn=o(),Ae=s("p"),Ae.innerHTML=es,mn=o(),P=s("h2"),P.innerHTML=ts,un=o(),S=s("h3"),S.innerHTML=ns,yn=o(),Me=s("p"),Me.textContent=as,vn=o(),Ie=s("p"),Ie.innerHTML=ss,gn=o(),Pe=s("p"),Pe.innerHTML=is,Dn=o(),Se=s("p"),Se.textContent=ls,wn=o(),Re=s("p"),Re.textContent=os,xn=o(),ze=s("p"),ze.textContent=rs,bn=o(),je=s("p"),je.textContent=ps,Cn=o(),qe=s("p"),qe.textContent=cs,En=o(),Be=s("p"),Be.textContent=hs,_n=o(),R=s("h3"),R.innerHTML=ds,kn=o(),Ne=s("p"),Ne.textContent=fs,Ln=o(),Ge=s("p"),Ge.textContent=Fs,Tn=o(),We=s("p"),We.textContent=ms,Hn=o(),Oe=s("p"),Oe.textContent=us,An=o(),z=s("h3"),z.innerHTML=ys,Mn=o(),Ue=s("p"),Ue.textContent=vs,In=o(),j=s("h1"),j.innerHTML=gs,Pn=o(),Ve=s("p"),Ve.textContent=Ds,Sn=o(),Ye=s("center"),Ye.innerHTML=ws,Rn=Ms(`
For inference I used:
`),Ke=s("ul"),Ke.innerHTML=xs,zn=o(),Xe=s("ul"),Xe.innerHTML=bs,jn=o(),$e=s("p"),$e.innerHTML=Cs,qn=o(),Je=s("p"),Je.textContent=Es,Bn=o(),Ze=s("p"),Ze.textContent=_s,Nn=o(),Qe=s("p"),Qe.textContent=ks,Gn=o(),q=s("h1"),q.innerHTML=Ls,Wn=o(),et=s("p"),et.innerHTML=Ts,On=o(),B=s("div"),B.innerHTML=Hs,this.h()},l(e){c=i(e,"P",{"data-svelte-h":!0}),l(c)!=="svelte-76dcdd"&&(c.textContent=u),m=r(e),f=i(e,"UL",{"data-svelte-h":!0}),l(f)!=="svelte-ci61g1"&&(f.innerHTML=h),d=r(e),F=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(F)!=="svelte-l7r8fl"&&(F.innerHTML=Vn),lt=r(e),N=i(e,"P",{"data-svelte-h":!0}),l(N)!=="svelte-1xq9gil"&&(N.textContent=Yn),ot=r(e),G=i(e,"P",{"data-svelte-h":!0}),l(G)!=="svelte-19id2xe"&&(G.textContent=Kn),rt=r(e),W=i(e,"P",{"data-svelte-h":!0}),l(W)!=="svelte-e3yany"&&(W.innerHTML=Xn),pt=r(e),y=i(e,"H2",{id:!0,"data-svelte-h":!0}),l(y)!=="svelte-omv6yk"&&(y.innerHTML=$n),ct=r(e),O=i(e,"P",{"data-svelte-h":!0}),l(O)!=="svelte-z8e84b"&&(O.textContent=Jn),ht=r(e),U=i(e,"P",{"data-svelte-h":!0}),l(U)!=="svelte-1pjk3el"&&(U.textContent=Zn),dt=r(e),V=i(e,"P",{"data-svelte-h":!0}),l(V)!=="svelte-1iyjich"&&(V.innerHTML=Qn),ft=r(e),v=i(e,"H2",{id:!0,"data-svelte-h":!0}),l(v)!=="svelte-1gtbi8s"&&(v.innerHTML=ea),Ft=r(e),Y=i(e,"P",{"data-svelte-h":!0}),l(Y)!=="svelte-wypfys"&&(Y.innerHTML=ta),mt=r(e),K=i(e,"CENTER",{"data-svelte-h":!0}),l(K)!=="svelte-13el4n1"&&(K.innerHTML=na),ut=r(e),g=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(g)!=="svelte-6e6w1w"&&(g.innerHTML=aa),yt=r(e),X=i(e,"P",{"data-svelte-h":!0}),l(X)!=="svelte-1y9bpgj"&&(X.innerHTML=sa),vt=r(e),$=i(e,"P",{"data-svelte-h":!0}),l($)!=="svelte-t2t6j3"&&($.textContent=ia),gt=r(e),J=i(e,"P",{"data-svelte-h":!0}),l(J)!=="svelte-1gxu2y"&&(J.textContent=la),Dt=r(e),D=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(D)!=="svelte-emq2ou"&&(D.innerHTML=oa),wt=r(e),Z=i(e,"P",{"data-svelte-h":!0}),l(Z)!=="svelte-5xnk0t"&&(Z.textContent=ra),xt=r(e),Q=i(e,"CENTER",{"data-svelte-h":!0}),l(Q)!=="svelte-28zbkm"&&(Q.innerHTML=pa),bt=Ps(e,`
I don't want to deep dive into how Stable Diffusion works but there are great resources that I can suggest for you:
`),ee=i(e,"UL",{"data-svelte-h":!0}),l(ee)!=="svelte-16lzsga"&&(ee.innerHTML=ca),Ct=r(e),te=i(e,"P",{"data-svelte-h":!0}),l(te)!=="svelte-4nptg8"&&(te.textContent=ha),Et=r(e),ne=i(e,"UL",{"data-svelte-h":!0}),l(ne)!=="svelte-nsxr31"&&(ne.innerHTML=da),_t=r(e),ae=i(e,"P",{"data-svelte-h":!0}),l(ae)!=="svelte-12f7wfc"&&(ae.textContent=fa),kt=r(e),w=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(w)!=="svelte-1mnrtt0"&&(w.innerHTML=Fa),Lt=r(e),se=i(e,"P",{"data-svelte-h":!0}),l(se)!=="svelte-48hwb"&&(se.textContent=ma),Tt=r(e),ie=i(e,"P",{"data-svelte-h":!0}),l(ie)!=="svelte-1952n39"&&(ie.textContent=ua),Ht=r(e),x=i(e,"H4",{id:!0,"data-svelte-h":!0}),l(x)!=="svelte-1piw1pi"&&(x.innerHTML=ya),At=r(e),le=i(e,"P",{"data-svelte-h":!0}),l(le)!=="svelte-zov7rk"&&(le.textContent=va),Mt=r(e),oe=i(e,"P",{"data-svelte-h":!0}),l(oe)!=="svelte-1mdkr6y"&&(oe.textContent=ga),It=r(e),b=i(e,"H4",{id:!0,"data-svelte-h":!0}),l(b)!=="svelte-b2cs75"&&(b.innerHTML=Da),Pt=r(e),re=i(e,"P",{"data-svelte-h":!0}),l(re)!=="svelte-15g3c30"&&(re.innerHTML=wa),St=r(e),pe=i(e,"P",{"data-svelte-h":!0}),l(pe)!=="svelte-1af92ux"&&(pe.textContent=xa),Rt=r(e),C=i(e,"H4",{id:!0,"data-svelte-h":!0}),l(C)!=="svelte-2l2n58"&&(C.innerHTML=ba),zt=r(e),ce=i(e,"P",{"data-svelte-h":!0}),l(ce)!=="svelte-1lltydh"&&(ce.innerHTML=Ca),jt=r(e),he=i(e,"P",{"data-svelte-h":!0}),l(he)!=="svelte-duhhst"&&(he.textContent=Ea),qt=r(e),E=i(e,"H2",{id:!0,"data-svelte-h":!0}),l(E)!=="svelte-6sjifx"&&(E.innerHTML=_a),Bt=r(e),de=i(e,"P",{"data-svelte-h":!0}),l(de)!=="svelte-an45qf"&&(de.textContent=ka),Nt=r(e),_=i(e,"H4",{id:!0,"data-svelte-h":!0}),l(_)!=="svelte-69yijm"&&(_.innerHTML=La),Gt=r(e),fe=i(e,"P",{"data-svelte-h":!0}),l(fe)!=="svelte-1cdzlvm"&&(fe.textContent=Ta),Wt=r(e),k=i(e,"H4",{id:!0,"data-svelte-h":!0}),l(k)!=="svelte-1dodknv"&&(k.innerHTML=Ha),Ot=r(e),Fe=i(e,"P",{"data-svelte-h":!0}),l(Fe)!=="svelte-s1d1v7"&&(Fe.textContent=Aa),Ut=r(e),L=i(e,"H4",{id:!0,"data-svelte-h":!0}),l(L)!=="svelte-17nvmz"&&(L.innerHTML=Ma),Vt=r(e),me=i(e,"P",{"data-svelte-h":!0}),l(me)!=="svelte-gxerft"&&(me.textContent=Ia),Yt=r(e),T=i(e,"H4",{id:!0,"data-svelte-h":!0}),l(T)!=="svelte-xq211c"&&(T.innerHTML=Pa),Kt=r(e),ue=i(e,"P",{"data-svelte-h":!0}),l(ue)!=="svelte-1xojtvl"&&(ue.innerHTML=Sa),Xt=r(e),ye=i(e,"P",{"data-svelte-h":!0}),l(ye)!=="svelte-1g51nla"&&(ye.innerHTML=Ra),$t=r(e),H=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(H)!=="svelte-1l2bw8r"&&(H.innerHTML=za),Jt=r(e),ve=i(e,"P",{"data-svelte-h":!0}),l(ve)!=="svelte-yn09n2"&&(ve.innerHTML=ja),Zt=r(e),ge=i(e,"CENTER",{"data-svelte-h":!0}),l(ge)!=="svelte-1idxobw"&&(ge.innerHTML=qa),Qt=r(e),De=i(e,"P",{"data-svelte-h":!0}),l(De)!=="svelte-1tit0uw"&&(De.textContent=Ba),en=r(e),we=i(e,"P",{"data-svelte-h":!0}),l(we)!=="svelte-zq1wi1"&&(we.innerHTML=Na),tn=r(e),nt=Ss(e,!1),at=r(e),xe=i(e,"P",{"data-svelte-h":!0}),l(xe)!=="svelte-9e45zs"&&(xe.textContent=Ga),nn=r(e),st=Ss(e,!1),it=r(e),be=i(e,"P",{"data-svelte-h":!0}),l(be)!=="svelte-1rzvdbf"&&(be.textContent=Wa),an=r(e),Ce=i(e,"P",{"data-svelte-h":!0}),l(Ce)!=="svelte-3mvhc"&&(Ce.textContent=Oa),sn=r(e),A=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(A)!=="svelte-sffwxn"&&(A.innerHTML=Ua),ln=r(e),Ee=i(e,"P",{"data-svelte-h":!0}),l(Ee)!=="svelte-1hrj7xy"&&(Ee.innerHTML=Va),on=r(e),M=i(e,"H2",{id:!0,"data-svelte-h":!0}),l(M)!=="svelte-1sejxow"&&(M.innerHTML=Ya),rn=r(e),_e=i(e,"P",{"data-svelte-h":!0}),l(_e)!=="svelte-f9edud"&&(_e.innerHTML=Ka),pn=r(e),ke=i(e,"P",{"data-svelte-h":!0}),l(ke)!=="svelte-18q42j4"&&(ke.textContent=Xa),cn=r(e),I=i(e,"H2",{id:!0,"data-svelte-h":!0}),l(I)!=="svelte-14j0ks0"&&(I.innerHTML=$a),hn=r(e),Le=i(e,"P",{"data-svelte-h":!0}),l(Le)!=="svelte-f8oh7c"&&(Le.innerHTML=Ja),dn=r(e),Te=i(e,"CENTER",{"data-svelte-h":!0}),l(Te)!=="svelte-17jryi"&&(Te.innerHTML=Za),fn=r(e),He=i(e,"P",{"data-svelte-h":!0}),l(He)!=="svelte-50z69l"&&(He.innerHTML=Qa),Fn=r(e),Ae=i(e,"P",{"data-svelte-h":!0}),l(Ae)!=="svelte-dd9pxj"&&(Ae.innerHTML=es),mn=r(e),P=i(e,"H2",{id:!0,"data-svelte-h":!0}),l(P)!=="svelte-1a42899"&&(P.innerHTML=ts),un=r(e),S=i(e,"H3",{id:!0,"data-svelte-h":!0}),l(S)!=="svelte-g6o2fl"&&(S.innerHTML=ns),yn=r(e),Me=i(e,"P",{"data-svelte-h":!0}),l(Me)!=="svelte-jubdze"&&(Me.textContent=as),vn=r(e),Ie=i(e,"P",{"data-svelte-h":!0}),l(Ie)!=="svelte-wn8m8a"&&(Ie.innerHTML=ss),gn=r(e),Pe=i(e,"P",{"data-svelte-h":!0}),l(Pe)!=="svelte-7ss2tj"&&(Pe.innerHTML=is),Dn=r(e),Se=i(e,"P",{"data-svelte-h":!0}),l(Se)!=="svelte-d8n9bc"&&(Se.textContent=ls),wn=r(e),Re=i(e,"P",{"data-svelte-h":!0}),l(Re)!=="svelte-wraidd"&&(Re.textContent=os),xn=r(e),ze=i(e,"P",{"data-svelte-h":!0}),l(ze)!=="svelte-14xjptv"&&(ze.textContent=rs),bn=r(e),je=i(e,"P",{"data-svelte-h":!0}),l(je)!=="svelte-vq5j03"&&(je.textContent=ps),Cn=r(e),qe=i(e,"P",{"data-svelte-h":!0}),l(qe)!=="svelte-le9wnw"&&(qe.textContent=cs),En=r(e),Be=i(e,"P",{"data-svelte-h":!0}),l(Be)!=="svelte-1utr4tu"&&(Be.textContent=hs),_n=r(e),R=i(e,"H3",{id:!0,"data-svelte-h":!0}),l(R)!=="svelte-1unxndu"&&(R.innerHTML=ds),kn=r(e),Ne=i(e,"P",{"data-svelte-h":!0}),l(Ne)!=="svelte-8k4f1a"&&(Ne.textContent=fs),Ln=r(e),Ge=i(e,"P",{"data-svelte-h":!0}),l(Ge)!=="svelte-mp3fq0"&&(Ge.textContent=Fs),Tn=r(e),We=i(e,"P",{"data-svelte-h":!0}),l(We)!=="svelte-3jwbjp"&&(We.textContent=ms),Hn=r(e),Oe=i(e,"P",{"data-svelte-h":!0}),l(Oe)!=="svelte-vs9pe3"&&(Oe.textContent=us),An=r(e),z=i(e,"H3",{id:!0,"data-svelte-h":!0}),l(z)!=="svelte-h48yzs"&&(z.innerHTML=ys),Mn=r(e),Ue=i(e,"P",{"data-svelte-h":!0}),l(Ue)!=="svelte-pu4xd9"&&(Ue.textContent=vs),In=r(e),j=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(j)!=="svelte-1mea2hz"&&(j.innerHTML=gs),Pn=r(e),Ve=i(e,"P",{"data-svelte-h":!0}),l(Ve)!=="svelte-i0347t"&&(Ve.textContent=Ds),Sn=r(e),Ye=i(e,"CENTER",{"data-svelte-h":!0}),l(Ye)!=="svelte-ltf103"&&(Ye.innerHTML=ws),Rn=Ps(e,`
For inference I used:
`),Ke=i(e,"UL",{"data-svelte-h":!0}),l(Ke)!=="svelte-1cryeho"&&(Ke.innerHTML=xs),zn=r(e),Xe=i(e,"UL",{"data-svelte-h":!0}),l(Xe)!=="svelte-esytgg"&&(Xe.innerHTML=bs),jn=r(e),$e=i(e,"P",{"data-svelte-h":!0}),l($e)!=="svelte-19pf8j9"&&($e.innerHTML=Cs),qn=r(e),Je=i(e,"P",{"data-svelte-h":!0}),l(Je)!=="svelte-1oojx0n"&&(Je.textContent=Es),Bn=r(e),Ze=i(e,"P",{"data-svelte-h":!0}),l(Ze)!=="svelte-2tg79o"&&(Ze.textContent=_s),Nn=r(e),Qe=i(e,"P",{"data-svelte-h":!0}),l(Qe)!=="svelte-1ryvyx5"&&(Qe.textContent=ks),Gn=r(e),q=i(e,"H1",{id:!0,"data-svelte-h":!0}),l(q)!=="svelte-13qnvhc"&&(q.innerHTML=Ls),Wn=r(e),et=i(e,"P",{"data-svelte-h":!0}),l(et)!=="svelte-phek9b"&&(et.innerHTML=Ts),On=r(e),B=i(e,"DIV",{class:!0,"data-svelte-h":!0}),l(B)!=="svelte-16le02p"&&(B.innerHTML=Hs),this.h()},h(){p(F,"id","melanoma-skin-cancer"),p(y,"id","melanoma-in-people-of-color"),p(v,"id","key-indicators-of-melanoma"),p(g,"id","medical-image-synthesis"),p(D,"id","stable-diffusion"),p(w,"id","fine-tuning-methods-for-sd"),p(x,"id","traditional-naive-fine-tuning-nft"),p(b,"id","dreambooth"),p(C,"id","lora"),p(E,"id","comparison"),p(_,"id","computational-perspective"),p(k,"id","size-on-disk"),p(L,"id","why-i-chose-lora"),p(T,"id","fine-tuning-based-on-your-goal"),p(H,"id","data-preparation"),nt.a=at,st.a=it,p(A,"id","fine-tuning-sd-with-lora"),p(M,"id","base-model"),p(I,"id","note-on-beauty-standards-of-models"),p(P,"id","training-configs"),p(S,"id","lora-configs"),p(R,"id","optimizer-configs"),p(z,"id","general-config"),p(j,"id","results-and-discussion"),p(q,"id","references-and-notes"),p(B,"class","footnotes")},m(e,t){n(e,c,t),n(e,m,t),n(e,f,t),n(e,d,t),n(e,F,t),n(e,lt,t),n(e,N,t),n(e,ot,t),n(e,G,t),n(e,rt,t),n(e,W,t),n(e,pt,t),n(e,y,t),n(e,ct,t),n(e,O,t),n(e,ht,t),n(e,U,t),n(e,dt,t),n(e,V,t),n(e,ft,t),n(e,v,t),n(e,Ft,t),n(e,Y,t),n(e,mt,t),n(e,K,t),n(e,ut,t),n(e,g,t),n(e,yt,t),n(e,X,t),n(e,vt,t),n(e,$,t),n(e,gt,t),n(e,J,t),n(e,Dt,t),n(e,D,t),n(e,wt,t),n(e,Z,t),n(e,xt,t),n(e,Q,t),n(e,bt,t),n(e,ee,t),n(e,Ct,t),n(e,te,t),n(e,Et,t),n(e,ne,t),n(e,_t,t),n(e,ae,t),n(e,kt,t),n(e,w,t),n(e,Lt,t),n(e,se,t),n(e,Tt,t),n(e,ie,t),n(e,Ht,t),n(e,x,t),n(e,At,t),n(e,le,t),n(e,Mt,t),n(e,oe,t),n(e,It,t),n(e,b,t),n(e,Pt,t),n(e,re,t),n(e,St,t),n(e,pe,t),n(e,Rt,t),n(e,C,t),n(e,zt,t),n(e,ce,t),n(e,jt,t),n(e,he,t),n(e,qt,t),n(e,E,t),n(e,Bt,t),n(e,de,t),n(e,Nt,t),n(e,_,t),n(e,Gt,t),n(e,fe,t),n(e,Wt,t),n(e,k,t),n(e,Ot,t),n(e,Fe,t),n(e,Ut,t),n(e,L,t),n(e,Vt,t),n(e,me,t),n(e,Yt,t),n(e,T,t),n(e,Kt,t),n(e,ue,t),n(e,Xt,t),n(e,ye,t),n(e,$t,t),n(e,H,t),n(e,Jt,t),n(e,ve,t),n(e,Zt,t),n(e,ge,t),n(e,Qt,t),n(e,De,t),n(e,en,t),n(e,we,t),n(e,tn,t),nt.m(js,e,t),n(e,at,t),n(e,xe,t),n(e,nn,t),st.m(qs,e,t),n(e,it,t),n(e,be,t),n(e,an,t),n(e,Ce,t),n(e,sn,t),n(e,A,t),n(e,ln,t),n(e,Ee,t),n(e,on,t),n(e,M,t),n(e,rn,t),n(e,_e,t),n(e,pn,t),n(e,ke,t),n(e,cn,t),n(e,I,t),n(e,hn,t),n(e,Le,t),n(e,dn,t),n(e,Te,t),n(e,fn,t),n(e,He,t),n(e,Fn,t),n(e,Ae,t),n(e,mn,t),n(e,P,t),n(e,un,t),n(e,S,t),n(e,yn,t),n(e,Me,t),n(e,vn,t),n(e,Ie,t),n(e,gn,t),n(e,Pe,t),n(e,Dn,t),n(e,Se,t),n(e,wn,t),n(e,Re,t),n(e,xn,t),n(e,ze,t),n(e,bn,t),n(e,je,t),n(e,Cn,t),n(e,qe,t),n(e,En,t),n(e,Be,t),n(e,_n,t),n(e,R,t),n(e,kn,t),n(e,Ne,t),n(e,Ln,t),n(e,Ge,t),n(e,Tn,t),n(e,We,t),n(e,Hn,t),n(e,Oe,t),n(e,An,t),n(e,z,t),n(e,Mn,t),n(e,Ue,t),n(e,In,t),n(e,j,t),n(e,Pn,t),n(e,Ve,t),n(e,Sn,t),n(e,Ye,t),n(e,Rn,t),n(e,Ke,t),n(e,zn,t),n(e,Xe,t),n(e,jn,t),n(e,$e,t),n(e,qn,t),n(e,Je,t),n(e,Bn,t),n(e,Ze,t),n(e,Nn,t),n(e,Qe,t),n(e,Gn,t),n(e,q,t),n(e,Wn,t),n(e,et,t),n(e,On,t),n(e,B,t)},p:Ns,d(e){e&&(a(c),a(m),a(f),a(d),a(F),a(lt),a(N),a(ot),a(G),a(rt),a(W),a(pt),a(y),a(ct),a(O),a(ht),a(U),a(dt),a(V),a(ft),a(v),a(Ft),a(Y),a(mt),a(K),a(ut),a(g),a(yt),a(X),a(vt),a($),a(gt),a(J),a(Dt),a(D),a(wt),a(Z),a(xt),a(Q),a(bt),a(ee),a(Ct),a(te),a(Et),a(ne),a(_t),a(ae),a(kt),a(w),a(Lt),a(se),a(Tt),a(ie),a(Ht),a(x),a(At),a(le),a(Mt),a(oe),a(It),a(b),a(Pt),a(re),a(St),a(pe),a(Rt),a(C),a(zt),a(ce),a(jt),a(he),a(qt),a(E),a(Bt),a(de),a(Nt),a(_),a(Gt),a(fe),a(Wt),a(k),a(Ot),a(Fe),a(Ut),a(L),a(Vt),a(me),a(Yt),a(T),a(Kt),a(ue),a(Xt),a(ye),a($t),a(H),a(Jt),a(ve),a(Zt),a(ge),a(Qt),a(De),a(en),a(we),a(tn),nt.d(),a(at),a(xe),a(nn),st.d(),a(it),a(be),a(an),a(Ce),a(sn),a(A),a(ln),a(Ee),a(on),a(M),a(rn),a(_e),a(pn),a(ke),a(cn),a(I),a(hn),a(Le),a(dn),a(Te),a(fn),a(He),a(Fn),a(Ae),a(mn),a(P),a(un),a(S),a(yn),a(Me),a(vn),a(Ie),a(gn),a(Pe),a(Dn),a(Se),a(wn),a(Re),a(xn),a(ze),a(bn),a(je),a(Cn),a(qe),a(En),a(Be),a(_n),a(R),a(kn),a(Ne),a(Ln),a(Ge),a(Tn),a(We),a(Hn),a(Oe),a(An),a(z),a(Mn),a(Ue),a(In),a(j),a(Pn),a(Ve),a(Sn),a(Ye),a(Rn),a(Ke),a(zn),a(Xe),a(jn),a($e),a(qn),a(Je),a(Bn),a(Ze),a(Nn),a(Qe),a(Gn),a(q),a(Wn),a(et),a(On),a(B))}}}function Qs(tt){let c,u;const m=[tt[0],zs];let f={$$slots:{default:[Zs]},$$scope:{ctx:tt}};for(let h=0;h<m.length;h+=1)f=Un(f,m[h]);return c=new $s({props:f}),{c(){Os(c.$$.fragment)},l(h){Us(c.$$.fragment,h)},m(h,d){Vs(c,h,d),u=!0},p(h,[d]){const F=d&1?Js(m,[d&1&&Rs(h[0]),d&0&&Rs(zs)]):{};d&2&&(F.$$scope={dirty:d,ctx:h}),c.$set(F)},i(h){u||(Ys(c.$$.fragment,h),u=!0)},o(h){Ks(c.$$.fragment,h),u=!1},d(h){Xs(c,h)}}}const zs={title:"Medical Image Synthesis with SDXL",summary:"How I Synthesized Medical Melanoma Images with Stable Diffusion and LoRa",created:"2023-11-02T00:00:00.000Z",updated:"2023-11-02T00:00:00.000Z",tags:["Stable-Diffusion","Medical","Image-Synthesis"],toc:[{depth:1,title:"Melanoma Skin Cancer",slug:"melanoma-skin-cancer"},{depth:2,title:"Melanoma in People of Color",slug:"melanoma-in-people-of-color"},{depth:2,title:"Key Indicators of Melanoma",slug:"key-indicators-of-melanoma"},{depth:1,title:"Medical Image Synthesis",slug:"medical-image-synthesis"},{depth:1,title:"Stable Diffusion",slug:"stable-diffusion"},{depth:1,title:"Fine-tuning Methods for SD",slug:"fine-tuning-methods-for-sd"},{depth:4,title:"Traditional Naive Fine-tuning (NFT)",slug:"traditional-naive-fine-tuning-nft"},{depth:4,title:"Dreambooth",slug:"dreambooth"},{depth:4,title:"LoRa",slug:"lora"},{depth:2,title:"Comparison",slug:"comparison"},{depth:4,title:"Computational Perspective",slug:"computational-perspective"},{depth:4,title:"Size on Disk",slug:"size-on-disk"},{depth:4,title:"Why I Chose LoRa",slug:"why-i-chose-lora"},{depth:4,title:"Fine-tuning Based on Your Goal",slug:"fine-tuning-based-on-your-goal"},{depth:1,title:"Data Preparation",slug:"data-preparation"},{depth:1,title:"Fine-tuning SD with LoRa",slug:"fine-tuning-sd-with-lora"},{depth:2,title:"Base Model",slug:"base-model"},{depth:2,title:"Note on Beauty Standards of Models",slug:"note-on-beauty-standards-of-models"},{depth:2,title:"Training Configs",slug:"training-configs"},{depth:3,title:"LoRa Configs",slug:"lora-configs"},{depth:3,title:"Optimizer Configs",slug:"optimizer-configs"},{depth:3,title:"General Config",slug:"general-config"},{depth:1,title:"Results and Discussion",slug:"results-and-discussion"},{depth:1,title:"References and Notes",slug:"references-and-notes"}],images:[],slug:"/synthetic-melanoma-images/+page.md",path:"/synthetic-melanoma-images"};function ei(tt,c,u){return tt.$$set=m=>{u(0,c=Un(Un({},c),As(m)))},c=As(c),[c]}class si extends Gs{constructor(c){super(),Ws(this,c,ei,Qs,Bs,{})}}export{si as component};
