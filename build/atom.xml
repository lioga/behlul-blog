<?xml version='1.0' encoding='utf-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://behlulakbudak.com/</id>
  <title><![CDATA[Behlul Akbudak]]></title>
  <subtitle><![CDATA[An Electronics Engineer Blog]]></subtitle>
  <icon>https://behlulakbudak.com/favicon.png</icon>
  <link href="https://behlulakbudak.com" />
  <link href="https://behlulakbudak.com/atom.xml" rel="self" type="application/atom+xml" />
  <updated>2023-11-07T18:47:23.525Z</updated>
  <author>
    <name><![CDATA[Behlul Akbudak]]></name>
  </author>
  <category term="Stable-Diffusion" scheme="https://behlulakbudak.com/?tags=Stable-Diffusion" />
  <category term="Medical" scheme="https://behlulakbudak.com/?tags=Medical" />
  <category term="Image-Synthesis" scheme="https://behlulakbudak.com/?tags=Image-Synthesis" />
  <category term="Drone" scheme="https://behlulakbudak.com/?tags=Drone" />
  <category term="Autonomous" scheme="https://behlulakbudak.com/?tags=Autonomous" />
  <category term="PCB" scheme="https://behlulakbudak.com/?tags=PCB" />
  <entry>
    <title type="html"><![CDATA[Medical Image Synthesis with SDXL]]></title>
    <link href="https://behlulakbudak.com/synthetic-melanoma-images" />
    <id>https://behlulakbudak.com/synthetic-melanoma-images</id>
    <published>2023-11-02T00:00:00.000Z</published>
    <updated>2023-11-02T00:00:00.000Z</updated>
    <summary type="html"><![CDATA[How I Synthesized Medical Melanoma Images with Stable Diffusion and LoRa]]></summary>
    <content type="html">
      <![CDATA[<p data-svelte-h="svelte-76dcdd">In this blog post, I will explain how I fine-tuned the “Stable Diffusion” with limited resources to generate synthetic malignant melanoma images. The goal of this project was to demonstrate that we can address class imbalance in medical datasets by generating synthetic data, ultimately enabling the training of more robust deep learning models.MelanomaWe will talk about:</p> <ul data-svelte-h="svelte-ci61g1"><li>Brief information about melanoma, role of synthetic data for augmenting rare medical images</li> <li>Quick overview of medical image synthesis</li> <li>Brief introduction to stable diffusion, what is new in SDXL</li> <li>Fine-tuning techniques for SDXL, comparison between them</li> <li>Preparing medical skin dataset and fine-tuning SD with LoRa</li> <li>Results and discussion</li></ul> <h1 id="melanoma-skin-cancer" data-svelte-h="svelte-l7r8fl"><a href="#melanoma-skin-cancer">Melanoma Skin Cancer</a></h1> <p data-svelte-h="svelte-1xq9gil">Melanoma, a form of skin cancer, arises when melanocytes, the cells responsible for skin pigmentation, undergo uncontrolled growth. This condition is a result of cells in the body losing control over their growth, a hallmark of cancer that can occur in various parts of the body and potentially spread to distant regions.</p> <p data-svelte-h="svelte-19id2xe">In many cases, melanoma cells continue to produce melanin, leading to tumors with brown or black pigmentation. However, some melanomas do not synthesize melanin, causing them to exhibit pink, tan, or even white coloration.</p> <p data-svelte-h="svelte-e3yany">Melanomas have the potential to develop anywhere on the skin, though they display a higher likelihood of originating on the trunk (chest and back) in men and on the legs in women. It’s important to note that having darkly pigmented skin does reduce the risk of developing melanoma at the more common sites. However, it’s crucial to emphasize that <strong>anyone</strong>, including individuals with darker skin, can indeed develop melanoma on the palms of their hands, the soles of their feet, or beneath their nails.</p> <h2 id="melanoma-in-people-of-color" data-svelte-h="svelte-omv6yk"><a href="#melanoma-in-people-of-color">Melanoma in People of Color</a></h2> <p data-svelte-h="svelte-z8e84b">It’s worth highlighting that melanoma diagnoses in Black individuals tend to occur at later stages, increasing the risk of severe cases. Darker skin contains a higher number of melanocytes, the pigment-producing cells that offer some level of protection against sun-induced skin damage, including the kind that leads to skin cancer. Darker skin is less prone to sunburn, which has sometimes led to the misconception that Black individuals are immune to skin cancer.</p> <p data-svelte-h="svelte-1pjk3el">This misconception can result in a lack of skin self-examinations for cancer, delayed medical consultations for skin changes. Moreover, even artificial intelligence systems designed to assist in early cancer detection may fail to adequately identify melanoma on darker skin if they lack a diverse dataset, including representative samples of individuals with darker skin tones.</p> <p data-svelte-h="svelte-1iyjich">To address this disparity, the use of synthetic data can play a valuable role in expanding the available data for melanoma detection in Black populations, improving diagnostic accuracy, and ultimately ensuring that all individuals, regardless of their skin tone, receive the appropriate care and attention in the fight against melanoma<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>.</p> <h2 id="key-indicators-of-melanoma" data-svelte-h="svelte-1gtbi8s"><a href="#key-indicators-of-melanoma">Key Indicators of Melanoma</a></h2> <p data-svelte-h="svelte-wypfys">Finding melanoma at an early stage is crucial; early detection can vastly increase your chances for cure. The first five letters of the alphabet are a guide to help you recognize the warning signs of melanoma. <strong>A is for Asymmetry</strong>, <strong>B is for Border</strong>, <strong>C is for Color</strong>, <strong>D is for Diameter or Dark</strong>, and <strong>E is for Evolving</strong>. More information can be found <a href="https://www.skincancer.org/skin-cancer-information/melanoma/melanoma-warning-signs-and-images/" rel="nofollow noopener noreferrer external" target="_blank">here</a>.</p> <center data-svelte-h="svelte-13el4n1"><img src="img/melanoma_detection.webp" alt="key indicators" width="430"></center> <h1 id="medical-image-synthesis" data-svelte-h="svelte-6e6w1w"><a href="#medical-image-synthesis">Medical Image Synthesis</a></h1> <p data-svelte-h="svelte-1y9bpgj">In the history of medical image synthesis, data scarcity in healthcare has driven the usage of generative models like Generative Adversarial Networks (GANs)<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup> and Variational Autoencoders (VAEs). GANs use a competitive game between a generator and discriminator to create increasingly realistic synthetic data, while VAEs learn a probabilistic mapping to balance data fidelity and diversity in their generated samples. You can find lots of resources on internet about how they work.</p> <p data-svelte-h="svelte-t2t6j3">GANs are powerful but troubled by learning instability, convergence issues, and mode collapse. On the other hand, VAEs excel in output diversity and avoid mode collapse, but often produce blurry and hazy images. Recent advances, such as denoising diffusion probabilistic models (DDPMs) and latent DDPMs, have outperformed other techniques in generating natural images. Thus, their performance in medical image synthesis is a current research topic.</p> <p data-svelte-h="svelte-1gxu2y">So, we will be exploring the capabilities of diffusion models, with a special focus on a particular model called the “Stable Diffusion”. This model has gained acclaim for its proficiency in generating highly realistic images.</p> <h1 id="stable-diffusion" data-svelte-h="svelte-emq2ou"><a href="#stable-diffusion">Stable Diffusion</a></h1> <p data-svelte-h="svelte-5xnk0t">Basically, Stable Diffusion is a text-to-image model that takes a prompt for input and then outputs an image that corresponds to that. First, they take input data and progressively introduce Gaussian noise in a controlled manner, creating a sort of “noisy” version of the data. This process is like a delicate recipe for noise creation. Then, the real power of these models comes into play. They are trained to reverse this diffusion process, effectively removing the noise and returning clean data from the initially noisy samples.</p> <center data-svelte-h="svelte-28zbkm"><img src="img/sd_arch.webp" alt="sd" width="480"></center>I don&#39;t want to deep dive into how Stable Diffusion works but there are great resources that I can suggest for you:<ul data-svelte-h="svelte-16lzsga"><li><a href="https://jalammar.github.io/illustrated-stable-diffusion/" rel="nofollow noopener noreferrer external" target="_blank">Jay Alammar</a> explains diffusion models and what is happening behind the stable diffusion with illustrations.</li> <li>You can check <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="nofollow noopener noreferrer external" target="_blank">Lilian Weng</a> if you want to learn math behind SD.</li> <li>What you are looking for is more practical knowledge, then SD <a href="https://www.reddit.com/r/StableDiffusion/" rel="nofollow noopener noreferrer external" target="_blank">subreddit</a> is way to go. You can find practical tips and more resources to get started.</li></ul> <p data-svelte-h="svelte-4nptg8">SDXL is the latest version of the stable diffusion. To highlight the differences from the previous version of SD, here’s a concise list:</p> <ul data-svelte-h="svelte-nsxr31"><li>Larger UNet backbone (three times larger)</li> <li>Increased model parameters with more attention blocks</li> <li>Introduction of a second text encoder</li> <li>Training on multiple aspect ratios</li> <li>Addition of a refinement model for improving visual fidelity</li></ul> <p data-svelte-h="svelte-12f7wfc">So, it’s undoubtedly more powerful and capable of generating fantastic results with even shorter prompts.</p> <h1 id="fine-tuning-methods-for-sd" data-svelte-h="svelte-1mnrtt0"><a href="#fine-tuning-methods-for-sd">Fine-tuning Methods for SD</a></h1> <p data-svelte-h="svelte-48hwb">Training large foundation models like Stable Diffusion from scratch is a formidable task due to their massive computational requirements, extended training times, and complex data needs. So, training these massive models from scratch is a pretty daunting task.</p> <p data-svelte-h="svelte-1952n39">That’s why many folks in the field are going for a more practical option. Instead of building everything from the ground up, they’re fine-tuning existing pretrained models. It’s a smart move because it saves time and resources and still gets you impressive results. However, there is not a single way to fine tune SD models.</p> <h4 id="traditional-naive-fine-tuning-nft" data-svelte-h="svelte-1piw1pi"><a href="#traditional-naive-fine-tuning-nft"><strong>Traditional Naive Fine-tuning (NFT)</strong></a></h4> <p data-svelte-h="svelte-zov7rk">This is classic fine-tuning for neural networks. Naive Fine-tuning neural networks means adjusting a pre-trained model for a new task, capitalizing on its prior knowledge while making it more suitable for the specific problem at hand. It alters the original model weights. New model can adapt highly specific use cases without prompt engineering.</p> <p data-svelte-h="svelte-1mdkr6y">However this process requires very large dataset, in terms of SD I recommend minimum dataset size of 1000, and you have to create complex captions for that dataset. Compared to other fine-tuning methods, this technique requires more training steps.</p> <h4 id="dreambooth" data-svelte-h="svelte-b2cs75"><a href="#dreambooth">Dreambooth</a></h4> <p data-svelte-h="svelte-15g3c30">Dreambooth<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup> approach is for “personalization” of text-to-image diffusion models. By providing a few reference images of a subject, the model fine-tunes itself to connect a unique identifier to that specific subject. Then, when you give it a text prompt, it can generate a range of lifelike images, each one capturing the essence of the subject, all while being versatile and creative in different contexts. More info can be found <a href="https://huggingface.co/blog/dreambooth" rel="nofollow noopener noreferrer external" target="_blank">here</a>.</p> <p data-svelte-h="svelte-1af92ux">For Dreambooth training you need to have a dataset of thing you want to train on also you need a regularization dataset. Regularization is used for making the model more robust for over-fitting. However, I have seen some folks that are not using it and still got great results. So, it is a bit trial and error with your dataset, which I think the most important aspect of training, and hyperparameter tuning such as finding a sweet spot for learning rate.</p> <h4 id="lora" data-svelte-h="svelte-2l2n58"><a href="#lora">LoRa</a></h4> <p data-svelte-h="svelte-1lltydh">LoRA<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup> (Low-Rank Adaptation) is a novel technique that originally invented for fine-tuning large-language models. It retains the pre-trained model’s core while incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This strategic move significantly reduces the number of parameters requiring fine-tuning and, notably, alleviates the GPU memory burden.</p> <p data-svelte-h="svelte-duhhst">LoRA focuses on the cross-attention layers on the model, where textual and visual elements intersect. Researchers have found that by fine-tuning this specific aspect of the model, LoRA can achieve remarkable results, making it a compelling choice for optimizing large language models across a range of real-world tasks and applications. In order to train a LoRa, you need images of the subject and corresponding captions for them.</p> <h2 id="comparison" data-svelte-h="svelte-6sjifx"><a href="#comparison">Comparison</a></h2> <p data-svelte-h="svelte-an45qf">There are also other methods such as textual-inversion and hypernetworks. However, I think their performance are not suitable for medical image synthesizing. So, I will not mention them in this post. Also some people combine some methods together, for example, Dreambooth with textual inversion or LoRa training with Dreambooth style. I haven’t tried those myself so I don’t have any recommendations about that.</p> <h4 id="computational-perspective" data-svelte-h="svelte-69yijm"><a href="#computational-perspective">Computational Perspective</a></h4> <p data-svelte-h="svelte-1cdzlvm">If we compare these 3 methods in terms of computational intensity: Traditional Fine-tuning &gt; Dreambooth &gt; LoRa. You’ll require an A100 or a graphics card with at least 24GB of VRAM for the traditional approach. LoRa and Dreambooth, on the other hand, can be trained with 16GB of VRAM. With further optimization, it’s possible to train these methods on machines with 8-12GB of VRAM.</p> <h4 id="size-on-disk" data-svelte-h="svelte-1dodknv"><a href="#size-on-disk">Size on Disk</a></h4> <p data-svelte-h="svelte-s1d1v7">In terms of size, LoRa is the smallest among them. It is only 200 - 600MB in size generally. Other methods alters the original model and saves the whole weights. So they are bigger and take up few gigabytes on disk. This factor becomes important when you want to train different subjects or iterate over same subject with different parameters.</p> <h4 id="why-i-chose-lora" data-svelte-h="svelte-17nvmz"><a href="#why-i-chose-lora">Why I Chose LoRa</a></h4> <p data-svelte-h="svelte-gxerft">I chose to use LoRa for this task for some compelling reasons. First, LoRa is lightweight in terms of system requirements, and you can train it even with a free-tier Google Colab using a T4 GPU, making it accessible. It’s also speedy to train because it doesn’t alter the model’s original weights and focuses on a smaller set of parameters. Moreover, LoRa is much smaller in size. It is significantly smaller than traditional checkpoints. Given these advantages, I decided to test the performance of SDXL combined with LoRa in the field of medical imaging.</p> <h4 id="fine-tuning-based-on-your-goal" data-svelte-h="svelte-xq211c"><a href="#fine-tuning-based-on-your-goal">Fine-tuning Based on Your Goal</a></h4> <p data-svelte-h="svelte-1xojtvl">Ultimately, regardless of the chosen method, the specific aim you pursue determines the duration of training, the requisite hyperparameters, and the size of your dataset. The nature of what you aim to teach the model significantly influences these parameters. For instance, teaching a simple character might demand only 10-100 photos, while introducing intricate details in inference such as wings, requires more images and specific hyperparameters to match. Similarly, teaching a distinct style, like a self-created style such as ”<em>behlulism</em>,” necessitates a more extensive dataset.</p> <p data-svelte-h="svelte-1g51nla">In the case of medical images, the need for comprehensive data becomes apparent—gathering all available data becomes crucial for effective training in this domain. Because the base model knows nothing about it and you have to teach that concept. You might wonder, “Why fine-tune the model when it knows nothing about the subject I want to teach?” Well, in this approach, the magic lies in the pretrained weights. They equip the model not just with knowledge of your subject but also with a powerful understanding of what distinctly <em>isn’t</em> your subject.</p> <h1 id="data-preparation" data-svelte-h="svelte-1l2bw8r"><a href="#data-preparation">Data Preparation</a></h1> <p data-svelte-h="svelte-yn09n2">The dataset was generated by the International Skin Imaging Collaboration ISIC<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup> and they hosted a competition on Kaggle. In this competition, there were very few melanoma images compared to a ton of healthy ones – just 584 melanoma images and 32,542 healthy ones. To tackle this imbalance, the winning team had to bring in last year’s competition dataset because there simply weren’t enough melanoma images available<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>.</p> <center data-svelte-h="svelte-1idxobw"><img src="img/melanoma_samples.webp" alt="samples from dataset" width=""></center> <p data-svelte-h="svelte-1tit0uw">I wanted to use this dataset for fine-tuning SD with LoRa. In order to do that, I need to extract melanoma images from the dataset and also I have to create captions for that images. In CSV file, there is information about the name of the image, gender of the patient, age, and if the mole is malignant or not. I used that information for separating malignant images and captioning them.</p> <p data-svelte-h="svelte-zq1wi1">To start, I created a simple script that scans the <em>‘train.csv’</em> file to identify the malignant melanoma images and extracts them. After separating malignant images, I created another simple script for captioning:</p> <!-- HTML_TAG_START --><pre class="shiki material-default" python="true"><div class="language-id">python</div><div class='code-container'><code><div class='line'>def create_caption(row):</div><div class='line'>    """Create a caption for an image."""</div><div class='line'>    sex = row[2]</div><div class='line'>    anatom_site = row[4]</div><div class='line'>    age = row[3]</div><div class='line'>    benign_malignant = row[6]</div><div class='line'>    if benign_malignant.lower() == 'malignant':</div><div class='line'>        sex_caption = sex or ""</div><div class='line'>        anatom_site_caption = f"on &#123;anatom_site&#125;" if anatom_site else ""</div><div class='line'>        age_caption = f", at age &#123;int(age)&#125;" if age else ""</div><div class='line'>        caption = f"photo of &#123;sex_caption&#125; skin, melanoma &#123;anatom_site_caption&#125;&#123;age_caption&#125;"</div><div class='line'>        return caption</div><div class='line'>    else:</div><div class='line'>        return None</div></code></div></pre><!-- HTML_TAG_END --> <p data-svelte-h="svelte-9e45zs">I utilized the data within the CSV file to generate diverse captions based on gender, age, and the anatomical site. Because I wanted more variation in captions, I created a script that analyzes if there is hair in the image.</p> <!-- HTML_TAG_START --><pre class="shiki material-default" python="true"><div class="language-id">python</div><div class='code-container'><code><div class='line'>def process_image(image_path):</div><div class='line'>"""Analyze the images if they are hairy or not."""</div><div class='line'>    # Load the image</div><div class='line'>    image = cv2.imread(image_path)</div><div class='line'>    # Convert the image to grayscale</div><div class='line'>    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</div><div class='line'>    # Apply Gaussian blur to reduce noise and improve contour detection</div><div class='line'>    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)</div><div class='line'>    # Use Canny edge detection to find edges in the image</div><div class='line'>    edges = cv2.Canny(blurred_image, 50, 250)</div><div class='line'>    # Find contours in the edge image</div><div class='line'>    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)</div><div class='line'>    # Define a minimum contour length threshold to filter out small artifacts</div><div class='line'>    min_contour_length = 100  </div><div class='line'>    # Filter out small contours (artifacts)</div><div class='line'>    filtered_contours = [contour for contour in contours if cv2.arcLength(contour, False) &gt;= min_contour_length]</div><div class='line'>    total_perimeter = 0</div><div class='line'>    for contour in filtered_contours:</div><div class='line'>        perimeter = cv2.arcLength(contour, False)</div><div class='line'>        total_perimeter += perimeter</div><div class='line'>    print(f'Total Perimeter: &#123;total_perimeter&#125;')</div><div class='line'>    name_of_caption = image_path.split(".")</div><div class='line'>    # If the total perimeter is greater than 4000, add ', hairy' to the caption</div><div class='line'>    if total_perimeter &gt; 4000:</div><div class='line'>        # Add ', hairy' to the caption file associated with this image</div><div class='line'>        caption_filename = name_of_caption[0] + '.caption'</div><div class='line'>        with open(caption_filename, 'a') as file:</div><div class='line'>            text_to_add = ', hairy'</div><div class='line'>            file.write(text_to_add)</div></code></div></pre><!-- HTML_TAG_END --> <p data-svelte-h="svelte-1rzvdbf">I think the code is quite self explanatory. Essentially, it identifies edge counters in the images, summing them up to compare against a predefined threshold to determine if there’s sufficient hair in the image. When hair is detected, it appends a “hairy” caption to the existing files. It’s worth noting that the script is experimental and not without its imperfections, mainly because of the dynamic nature of medical images. That’s why I decided to keep it separate from the original captioning script.</p> <p data-svelte-h="svelte-3mvhc">Finally, the created captions will be in the format of:photo of a (sex) skin, me1anoma on (anatom_site), at age (age_approx), (hairy).</p> <h1 id="fine-tuning-sd-with-lora" data-svelte-h="svelte-sffwxn"><a href="#fine-tuning-sd-with-lora">Fine-tuning SD with LoRa</a></h1> <p data-svelte-h="svelte-1hrj7xy">For fine-tuning purposes, I employed the scripts available from <a href="https://github.com/kohya-ss/sd-scripts" rel="nofollow noopener noreferrer external" target="_blank">kohya_ss</a>. Alternatively, you can also use the <a href="https://github.com/huggingface/diffusers/" rel="nofollow noopener noreferrer external" target="_blank">diffusers</a> library from Hugging Face. The kohya_ss repository offers various versions and experimental models, making it worth exploring.</p> <h2 id="base-model" data-svelte-h="svelte-1sejxow"><a href="#base-model">Base Model</a></h2> <p data-svelte-h="svelte-f9edud">To begin with, LoRa necessitates a base model for training. There are two approaches regarding the choice of base model. Some people advocate consistently using standard checkpoints (pretrained model weights) like SD 1.5 or SDXL 1.0. Others suggest selecting a checkpoint aligned with your specific objectives. For example, if your aim is to generate highly realistic images of the character you’re training on, a specialized checkpoint with a focus on realism, such as the one provided <a href="https://civitai.com/models/152525/realism-engine-sdxl" rel="nofollow noopener noreferrer external" target="_blank">here</a>, should be your choice. On the other hand, if you want to create anime-like images, a base model specialized for anime generation is preferable.</p> <p data-svelte-h="svelte-18q42j4">Both approaches make sense, and your choice really depends on what you’re trying to achieve. If you want to transfer yourself or a real world person into the world of SD, then those checkpoints with a focus on realism are great because they understand how real people look and give you a head start while fine-tuning. However, if you want to capture a specific style or something unique, like medical images, it’s better to go with base models like SDXL 1.0 or SD 1.5. They’re less biased towards any specific outcome.</p> <h2 id="note-on-beauty-standards-of-models" data-svelte-h="svelte-14j0ks0"><a href="#note-on-beauty-standards-of-models">Note on Beauty Standards of Models</a></h2> <p data-svelte-h="svelte-f8oh7c">I want to highlight that these models often aim to produce images that align with what are considered ‘stereotypical beauty standards’, focusing on achieving ‘perfect’ body and facial features. During my experimentation with these models and fine-tuning processes, I wanted to transfer my girlfriend to the world of SD. She has a beautiful feature, a charming nasal hump on her nose. However, when generating images, we often struggled to retain this nasal hump in the resulting images because the model tends to prioritize conforming to these conventional beauty norms. In response to this, there are even checkpoints available that specifically aim to depict more natural <a href="https://civitai.com/models/98755/humans" rel="nofollow noopener noreferrer external" target="_blank">humans</a>.</p> <center data-svelte-h="svelte-17jryi"><img src="img/loratestresult.webp" alt="lora results" width=""></center> <p data-svelte-h="svelte-50z69l"><em>One of the best resuls we achieved; left real right synthetic</em></p> <p data-svelte-h="svelte-dd9pxj">I’d love to delve deeper into this topic and explore the biases in these models, for example race and gender discrimination is hugely biased in these models, but that would be a whole new blog subject. In the meantime, you can check out <a href="https://www.bloomberg.com/graphics/2023-generative-ai-bias/" rel="nofollow noopener noreferrer external" target="_blank">this article</a>.</p> <h2 id="training-configs" data-svelte-h="svelte-1a42899"><a href="#training-configs">Training Configs</a></h2> <h3 id="lora-configs" data-svelte-h="svelte-g6o2fl"><a href="#lora-configs">LoRa Configs</a></h3> <p data-svelte-h="svelte-jubdze">LoRa comes in various types, including LoRA-C3Lier, LoHa, and DyLoRA. Each type alters the kernel size of the 2D convolution layer and makes adjustments to the architecture. You can find more information on the internet. For this project, I’ll be using Kohya’s standard LoRa, which has been renamed as ‘LoRA-LierLa’. Two specific configurations need to be set for this model:</p> <p data-svelte-h="svelte-wn8m8a"><strong>network_dim:</strong> The network dimensions, or rank, indicates how many parameters of the Unet/TE to train. The higher the value, the greater the “expressive power” of the model, but at the expense of larger file size. Most of the time 64 is suffice if you want to train something generic. However, when dealing with more specialized subjects 128 is a better choice. In the case of medical images, I choose my rank as 128.</p> <p data-svelte-h="svelte-7ss2tj"><strong>network_alpha:</strong> Network alpha is like a learning control knob – it’s there to put the brakes on or, in other words, dampen the learning process. Essentially, it’s used to adjust the model’s weights during saving to prevent any rounding errors that might mess up some of the weight values.Setting alpha to 0 or making it equal to network dimensions (net dim) doesn’t apply any dampening. However, when set to 1, it introduces significant dampening, requiring more training steps or a higher learning rate. A comparison of different scaling options is provided below:</p> <p data-svelte-h="svelte-d8n9bc">Alpha 0 = Alpha 128 = 128/128 = x1</p> <p data-svelte-h="svelte-wraidd">Alpha 1 = 1/128 = x0.0078125</p> <p data-svelte-h="svelte-14xjptv">Alpha 64 = 64/128 = x0.5</p> <p data-svelte-h="svelte-vq5j03">Alpha 128 = 128/128 = x1</p> <p data-svelte-h="svelte-le9wnw">Alpha 256 = 256/128 = x2</p> <p data-svelte-h="svelte-1utr4tu">When in doubt, you can set it equal to the network_dim.</p> <h3 id="optimizer-configs" data-svelte-h="svelte-1unxndu"><a href="#optimizer-configs">Optimizer Configs</a></h3> <p data-svelte-h="svelte-8k4f1a">In my opinion, AdamW stands out as one of the best optimizers available for this task, provided your system can handle the computational cost. If you’re working with budget constraints, an alternative optimizer to consider is Adafactor. It uses less memory by only storing partial information from previous gradients. Another solid choice is the 8-bit version of AdamW, known as AdamW8bit.</p> <p data-svelte-h="svelte-mp3fq0">As a note, AdamW8bit performed better than Adafactor in my case. However, I encourage you to experiment and explore options to see what works best for your particular project.</p> <p data-svelte-h="svelte-3jwbjp">As learning rate, I think it is also highly dependent on what you are trying to achieve. I have seen that the value of it can range anywhere from 4e-7, which is the standard learning rate for SDXL, to 8e-5. It is highly dependent on the dataset. If you are trying to teach a usual concept like a person, it is safe to increase it. However, if you are trying to teach a more unique concept, like medical images, lowering it down will be more beneficial. And don’t forget that as you decrease the learning rate, you have to train the model longer.</p> <p data-svelte-h="svelte-vs9pe3">I use a constant learning rate scheduler with a warm-up period of 100 steps. While some people recommend the cosine learning rate scheduler for better performance, I believe it can be a bit of a trial-and-error process to find the right fit for your dataset.</p> <h3 id="general-config" data-svelte-h="svelte-h48yzs"><a href="#general-config">General Config</a></h3> <p data-svelte-h="svelte-pu4xd9">For my other configurations, I typically set the number of repeats to 1 while aiming to maximize the number of epochs. There’s a parameter that determines how frequently the training script saves snapshots of the current model status. This periodic saving also triggers an inference run on the model with a predefined input prompt for testing. I monitor the generated images closely to determine whether LoRa is ready for the task. If it starts producing images with noticeable artifacts, it’s usually a sign of overfitting.</p> <h1 id="results-and-discussion" data-svelte-h="svelte-1mea2hz"><a href="#results-and-discussion">Results and Discussion</a></h1> <p data-svelte-h="svelte-i0347t">I trained several LoRa models. I used different learning rates, optimizers, and schedulers that I talked about through this blog post. Finally, I achieved some satisfactory results.</p> <center data-svelte-h="svelte-ltf103"><img src="img/melanomasynthetic.webp" alt="synthetic melanoma" width="480"></center>For inference I used:<ul data-svelte-h="svelte-1cryeho"><li>Sampler: Eular a, DPM++ 2M/3M Karras</li></ul> <ul data-svelte-h="svelte-esytgg"><li>20-30 Sampling steps</li> <li>7.5-9 CFG Scale</li></ul> <p data-svelte-h="svelte-19pf8j9">When I applied to the ABCDE rule that we talked about in <a href="#medical-image-synthesis">medical images section</a> to the synthesized images, I have seen that the most of the generations passed the test. However, when I looked at carefully for some images, I have seen that some of them are little blurry like the lower right corner. I think the reason for that could be the dataset.</p> <p data-svelte-h="svelte-1oojx0n">Following that, I decided to present a set of synthetic images to a group of professionals I am acquainted with. I explained the process and asked them to distinguish the synthetic images from the real ones. After examining the images, we engaged in a discussion about their observations and methodology. The professionals mentioned that they often perceived an image as synthetic when it exhibited fewer details. However, there was a complication as some real images in the dataset were quite simple and were mistakenly labeled as synthetic. Additionally, they encountered challenges when some complex synthetic images, which included elements like hair or red dots, were incorrectly identified as real by the examiners.</p> <p data-svelte-h="svelte-2tg79o">The results are promising, though not perfect at this stage. To further this project, collaboration with pathologists and dermatologists is essential. I firmly believe that the application of diffusion models in generating synthetic medical images holds great potential in this field.</p> <p data-svelte-h="svelte-1ryvyx5">Remarkably, I find it quite astonishing that by simply fine-tuning with LoRa, we are able to generate medical images. This process has shown a realm of new exciting possibilities for me. It’s especially significant due to the remarkably low costs involved, both in terms of system resources and time. Additionally, the compact model weights make it convenient to store and re-train various LoRas, offering substantial versatility for various applications.</p> <h1 id="references-and-notes" data-svelte-h="svelte-13qnvhc"><a href="#references-and-notes">References and Notes</a></h1> <p data-svelte-h="svelte-phek9b">All the code I wrote can be found in this <a href="https://github.com/lioga/melanoma-stable-diffusion" rel="nofollow noopener noreferrer external" target="_blank">repo</a>.</p> <div class="footnotes" data-svelte-h="svelte-16le02p"><hr> <ol><li id="fn-1">In order to achieve that task at least some amount of data is required to train generative AI model.<a href="#fnref-1" class="footnote-backref">↩</a></li> <li id="fn-2">Chen, R.J., Lu, M.Y., Chen, T.Y. <em>et al.</em> Synthetic data in machine learning for medicine and healthcare. <em>Nat Biomed Eng</em> <strong>5</strong>, 493–497 (2021). <a href="https://doi.org/10.1038/s41551-021-00751-8" rel="nofollow noopener noreferrer external" target="_blank">https://doi.org/10.1038/s41551-021-00751-8</a><a href="#fnref-2" class="footnote-backref">↩</a></li> <li id="fn-3"><a href="https://arxiv.org/abs/2208.12242" rel="nofollow noopener noreferrer external" target="_blank">https://arxiv.org/abs/2208.12242</a><a href="#fnref-3" class="footnote-backref">↩</a></li> <li id="fn-4"><a href="https://arxiv.org/abs/2106.09685" rel="nofollow noopener noreferrer external" target="_blank">https://arxiv.org/abs/2106.09685</a><a href="#fnref-4" class="footnote-backref">↩</a></li> <li id="fn-5">Rotemberg, V., Kurtansky, N., Betz-Stablein, B., Caffery, L., Chousakos, E., Codella, N., Combalia, M., Dusza, S., Guitera, P., Gutman, D., Halpern, A., Helba, B., Kittler, H., Kose, K., Langer, S., Lioprys, K., Malvehy, J., Musthaq, S., Nanda, J., Reiter, O., Shih, G., Stratigos, A., Tschandl, P., Weber, J. &amp; Soyer, P. A patient-centric dataset of images and metadata for identifying melanomas using clinical context. Sci Data 8, 34 (2021). <a href="https://doi.org/10.1038/s41597-021-00815-z" rel="nofollow noopener noreferrer external" target="_blank">https://doi.org/10.1038/s41597-021-00815-z</a><a href="#fnref-5" class="footnote-backref">↩</a></li> <li id="fn-6"><a href="https://arxiv.org/abs/2010.05351" rel="nofollow noopener noreferrer external" target="_blank">https://arxiv.org/abs/2010.05351</a><a href="#fnref-6" class="footnote-backref">↩</a></li></ol></div>]]>
    </content>
    <category term="Stable-Diffusion" scheme="https://behlulakbudak.com/?tags=Stable-Diffusion" />
    <category term="Medical" scheme="https://behlulakbudak.com/?tags=Medical" />
    <category term="Image-Synthesis" scheme="https://behlulakbudak.com/?tags=Image-Synthesis" />
  </entry>
  <entry>
    <title type="html"><![CDATA[Synthetic Data | Overview]]></title>
    <link href="https://behlulakbudak.com/synthetic-data" />
    <id>https://behlulakbudak.com/synthetic-data</id>
    <published>2023-10-19T00:00:00.000Z</published>
    <updated>2023-10-19T00:00:00.000Z</updated>
    <summary type="html"><![CDATA[What is Synthetic Data, Where to Use It and How to Generate It]]></summary>
    <content type="html">
      <![CDATA[<h1 id="what-is-synthetic-data" data-svelte-h="svelte-geelvs"><a href="#what-is-synthetic-data">What is Synthetic Data?</a></h1> <p data-svelte-h="svelte-148t9cu">Synthetic data is essentially a fabricated dataset generated by training a computer model to replicate the key features and structure of the original data. In simpler terms, the synthetic data is a fake data that is generated with various techniques based on the real data. This approach can be applied to generate synthetic text, videos, images, or even sound.</p> <h1 id="where-is-synthetic-data-used" data-svelte-h="svelte-1kwziz8"><a href="#where-is-synthetic-data-used">Where is Synthetic Data Used?</a></h1> <p data-svelte-h="svelte-1l7py8b">Synthetic data serves diverse purposes across multiple industries. Within manufacturing and quality control, synthetic data aids in simulating product scenarios and defects, training machine vision systems to enhance quality assessment. In robotics, synthetic data plays a pivotal role in robot training and simulation, enabling robots to acquire competence in various tasks and environments prior to real-world deployment. In healthcare, synthetic medical imagery and patient data facilitate research, testing of medical imaging equipment, and training machine learning models while preserving patient privacy.</p> <h1 id="why-synthetic-data-is-important" data-svelte-h="svelte-vo5pw"><a href="#why-synthetic-data-is-important">Why Synthetic Data is Important?</a></h1> <p data-svelte-h="svelte-10u8km2">Why do we require synthetic data? The real world might offer a vast expanse of data, but it’s not always readily accessible. The hurdles are numerous: it can be slow, costly, and often lacks the diversity needed for robust AI training. Achieving a diverse dataset means capturing data across a multitude of environments, lighting conditions, colors, and objects. In contrast, the digital realm allows for the manipulation of these variables from the comfort of one’s chair.</p> <p data-svelte-h="svelte-1ievqpv">Moreover, synthetic data addresses the challenge of class imbalance in real-world datasets, allowing for the generation of additional samples to balance the representation of different classes, thus improving the performance of AI models across diverse scenarios. Additionally, synthetic data enables the simulation of edge scenarios, enabling researchers and developers to test models against rare and critical events, ensuring the readiness of AI systems for a wide range of situations.</p> <p data-svelte-h="svelte-15fh5oe">Privacy is another concern. Many organizations and researchers grapple with the challenge of safeguarding sensitive information. Using real-world data can severely limit flexibility in data usage. Synthetic data, however, can be shared more liberally, promoting collaboration and insights without breaching privacy regulations. Researchers can exchange synthetic datasets without the encumbrances of violating stringent data protection regulations, such as the European Union’s GDPR or the U.S.’s HIPAA, which impose rigorous standards for safeguarding personal data. In this context, synthetic data becomes a valuable tool for organizations and researchers to maintain compliance, enhance model robustness, and mitigate the risk of data breaches.</p> <h1 id="how-to-generate-synthetic-data" data-svelte-h="svelte-1wjzboh"><a href="#how-to-generate-synthetic-data">How to Generate Synthetic Data</a></h1> <p data-svelte-h="svelte-lltuqb">There are numerous ways to generate different types of synthetic data exists, catering to various data types like text, videos, images, and audio. While my expertise primarily lies in image synthesis, and the rest of the blog will be about synthetic image generation, I’d like to provide you with valuable starting points for working with other types of synthetic data:</p> <ul data-svelte-h="svelte-1rslhu9"><li><strong>Text:</strong> One of the most educational videos on LLMs is Andrej Karpathy’s ‘GPT from scratch’ <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" rel="nofollow noopener noreferrer external" target="_blank">video</a>. Additionally, for fine-tuning pre-trained models, you might explore the capabilities of LLaMA 2.</li> <li><strong>Video</strong>: The field of synthetic video generation is continuously evolving, with ongoing advancements. A notable example is the work by NVIDIA researchers, who have achieved remarkable results. For more insights, you can refer to their <a href="https://research.nvidia.com/labs/toronto-ai/VideoLDM/" rel="nofollow noopener noreferrer external" target="_blank">research</a>.</li> <li><strong>Music</strong>: Similar to video generation, synthetic music production is an area that still holds untapped potential. Some of the works I follow are Stability AI’s ’<a href="https://stability.ai/stable-audio" rel="nofollow noopener noreferrer external" target="_blank">Stable Audio</a>’ and another <a href="https://github.com/Fansesi/Tubitak2204A_2023" rel="nofollow noopener noreferrer external" target="_blank">research</a> that is dedicated to synthesizing classical guitar pieces specifically.</li></ul> <p data-svelte-h="svelte-6i2au1">For image generation I’d like to explore different approaches that I used.</p> <h2 id="game-engine-method" data-svelte-h="svelte-1ufnyyp"><a href="#game-engine-method">Game Engine Method</a></h2> <p data-svelte-h="svelte-4qb5u9">These engines offer two valuable advantages. Firstly, they provide the capability to render highly realistic characters and scenes, which can be leveraged to gather data for your deep learning models. Moreover, these engines offer the convenience of arranging scenes according to your specific requirements without leaving your desk. However, it’s worth noting that mastering these engines can be a lengthy and demanding process.</p> <p data-svelte-h="svelte-i2tqo6">Secondly, these engines offer the invaluable advantage of providing precise labels, which are fundamental for training deep learning models. For instance, Unreal Engine supports <a href="https://docs.unrealengine.com/5.3/en-US/cinematic-render-passes-in-unreal-engine/" rel="nofollow noopener noreferrer external" target="_blank">render passes</a> that can automatically generate instance segmentation, saving you time and effort. Additionally, you can find various plugins designed specifically for facilitating this process.</p> <p><img src="img/unreal.webp" alt="unreal" class="rounded-lg my-2" loading="lazy" decoding="async">Similarly, Unity has its built-in <a href="https://unity.com/products/computer-vision" rel="nofollow noopener noreferrer external" target="_blank" data-svelte-h="svelte-1fo4dwe">computer vision tool</a>, allowing you to extract high-quality labels and data for your machine learning tasks with ease. Furthermore, NVIDIA’s offering, known as <a href="https://developer.nvidia.com/isaac-sim" rel="nofollow noopener noreferrer external" target="_blank" data-svelte-h="svelte-syandh">Isaac Sim</a>, is a comprehensive solution scaled on Omniverse. It’s particularly well-suited for generating synthetic images and training robots.</p> <p><img src="img/unity.webp" alt="unity" class="rounded-lg my-2" loading="lazy" decoding="async"></p> <p data-svelte-h="svelte-1tspdff">Another fantastic tool worth exploring is Blender, a robust open-source software. Many individuals are leveraging its capabilities for impressive 3D work. Take a look at this <a href="https://conference.blender.org/2022/presentations/1390/" rel="nofollow noopener noreferrer external" target="_blank">conference talk</a> as an example of the synthetic data generation in Blender.</p> <h2 id="generative-ai-method" data-svelte-h="svelte-1vm09dh"><a href="#generative-ai-method">Generative AI Method</a></h2> <p data-svelte-h="svelte-987epa">Generative AI has come a long way in the last couple of years. Now, you can create really realistic images with these models by just giving them a simple prompt. It’s become much easier to get good results quickly.</p> <p data-svelte-h="svelte-15d43bf">Various models have emerged during this time, but one of the most successful ones is the Stable Diffusion, especially with the recent addition of the <a href="https://stability.ai/blog/stable-diffusion-sdxl-1-announcement" rel="nofollow noopener noreferrer external" target="_blank">SDXL</a> model. With these models, you can easily generate batches of data for deep learning projects.</p> <center data-svelte-h="svelte-2pgfgl"><img src="img/sdxl.webp" width="500"></center> <p data-svelte-h="svelte-1v7qiyt">However, one downside of this method is control. Sometimes, it’s tricky to control specific elements, like poses, in these models. But researchers are actively working on this, and they’ve introduced something called <a href="https://github.com/lllyasviel/ControlNet" rel="nofollow noopener noreferrer external" target="_blank">ControlNet</a>. With ControlNet, you can give the model a guide, like an image that includes depth information or poses, and it’ll use that information to generate content that matches what you’re looking for.</p> <p data-svelte-h="svelte-qpthb5">While ControlNet provides control over the composition of generated images, it doesn’t offer the level of personalization one might desire. For instance, you can describe a character, generate an image you like, and then try generating again with the same description. Surprisingly, the result could be quite different, even if the described features are present in the final image. This undesired variability extends to styles as well.</p> <p data-svelte-h="svelte-49y85x">To address this issue, fine-tuning techniques have emerged as effective solutions. Methods like Dreambooth and LoRa allow you to personalize the images you generate, not only for characters but also for artistic styles. In fact, I have even explored using LoRa fine-tuning method to generate medical melanoma images with minimal resources, which you can read about <a href="https://behlulakbudak.com/synthetic-melanoma-images/" rel="nofollow noopener noreferrer external" target="_blank">here</a>.</p> <p data-svelte-h="svelte-1jaapqg">In conclusion, when it comes to generative AI, there remains a challenge in generating synthetic data for deep learning models used in various industries, particularly when aiming to create intricate scenes with complex objects. I think, this challenge persists, despite the capabilities offered by ControlNet and fine-tuning techniques.</p> <h2 id="game-engine--generative-ai-method" data-svelte-h="svelte-gh045a"><a href="#game-engine--generative-ai-method">Game Engine + Generative AI Method</a></h2> <p data-svelte-h="svelte-64u4cj">We’ve explored game engines and generative AI independently, but what about combining the two? By merging these technologies, we can effectively mitigate some of their respective limitations.</p> <p data-svelte-h="svelte-ua6sr7">Using game engines to create synthetic data has its challenges. While the latest Unreal Engine 5 might make it appear effortless to craft stunning scenes, the reality is that it still demands a substantial learning curve. It involves mastering a complex software, understanding principles like Physically Based Rendering (PBR) materials, crafting realistic lighting, and managing large assets that can strain your computer’s capabilities, among other intricacies.</p> <p data-svelte-h="svelte-kjy6nh">On the other hand, when you are solely using generative AI methods, you will not be able to generate perfect labels and, again, you have to label generated images. Moreover, when it comes to creating complex scenes, you might find yourself at a disadvantage, as you lack the reference data needed for precise control, especially when using models like ControlNet. While you could potentially source images from the internet and process them with tools like Midas to create depth maps, this approach is neither scalable nor sustainable for long-term work<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>.</p> <p data-svelte-h="svelte-s19gb9">The combination of game engines and generative AI can help address these shortcomings and unlock new possibilities in synthetic data generation.</p> <p data-svelte-h="svelte-123g52z"><strong>What you can do is:</strong></p> <ol data-svelte-h="svelte-2c6d4u"><li>Start by creating a basic scene (artists call that Blockout)</li> <li>Populate the scene with the objects of interest for training, alongside other elements that may act as distractors.</li> <li>Light the scene.</li> <li>Render the scene from your game engine to obtain precise labels for the objects. Generate instance and class segmentation masks.</li> <li>Capture different render passes like depth and normal map, which can be used with ControlNet.</li> <li>Use Stable Diffusion + ControlNet to bring your scene alive by inputting prompts</li></ol> <p data-svelte-h="svelte-ekfyqn"><strong>An example process:</strong></p> <ol data-svelte-h="svelte-1yyt09h"><li>Create a room and put a chair and a desk, add additional objects around, you could even add spheres, cubes…</li> <li>Place a scissors on the table</li> <li>Illuminate the scene and render it with labels, passes, and masks.</li> <li>Utilize Stable Diffusion and ControlNet to transform the scene into various settings such as an office space, a room belonging to a teenage girl, or a mystical witch’s lair…</li></ol> <center data-svelte-h="svelte-wrhucp"><img src="img/controlnet.webp" width="600"></center> <p data-svelte-h="svelte-1x3x24u">Possibilities are endless. You can get help from ChatGPT to find places where your objects are located.</p> <p data-svelte-h="svelte-q86adn">Finally, I can say this method is the most successful approach for generating synthetic data for non-abstract objects.</p> <h1 id="conclusion" data-svelte-h="svelte-dq8n5"><a href="#conclusion">Conclusion</a></h1> <p data-svelte-h="svelte-lk0hc5">In a nutshell, combining game engines with generative AI is a game-changer in the world of data generation for deep learning. Game engines like Unreal Engine and Unity offer realism and precise labeling, but come with a learning curve. On the other hand, generative AI, with models like Stable Diffusion and ControlNet, simplifies image generation but lacks fine-grained control.</p> <p data-svelte-h="svelte-ynx37j">When you bring these technologies together, magic happens. You start with a basic scene in a game engine, add objects, light it up, and get precise labels. Additional details like depth and normal maps are captured for extra control. Then, generative AI models like Stable Diffusion and ControlNet can breathe life into your scene.</p> <p data-svelte-h="svelte-1934qxi">This fusion streamlines the data generation process and gives you better control and precision. It’s a promising approach with potential across various industries. As both game engines and generative AI keep advancing, this synergy is set to redefine the future of deep learning and AI.</p> <h1 id="notes" data-svelte-h="svelte-1n5a1"><a href="#notes">Notes</a></h1> <div class="footnotes" data-svelte-h="svelte-12rm5r4"><hr> <ol><li id="fn-1">I’m aware that there are simple pose generators and various tools for integrating them with ControlNet. However, I think they can’t not fully replace the capabilities offered by comprehensive game engines.<a href="#fnref-1" class="footnote-backref">↩</a></li></ol></div>]]>
    </content>
    <category term="Drone" scheme="https://behlulakbudak.com/?tags=Drone" />
    <category term="Autonomous" scheme="https://behlulakbudak.com/?tags=Autonomous" />
    <category term="Image-Synthesis" scheme="https://behlulakbudak.com/?tags=Image-Synthesis" />
  </entry>
  <entry>
    <title type="html"><![CDATA[Creating PCBs at Home]]></title>
    <link href="https://behlulakbudak.com/pcb-manufacturing" />
    <id>https://behlulakbudak.com/pcb-manufacturing</id>
    <published>2023-10-18T00:00:00.000Z</published>
    <updated>2023-11-07T00:00:00.000Z</updated>
    <summary type="html"><![CDATA[PCB manufacturing with mixture of toner transfer and dry film methods]]></summary>
    <content type="html">
      <![CDATA[<h2 id="introduction" data-svelte-h="svelte-ccveyw"><a href="#introduction">Introduction</a></h2> <p data-svelte-h="svelte-1cwfn4n">The goal of this project is to produce PCBs at home, starting with simple boards and gradually advancing to more complex ones. The significance of this project lies in its ability to breathe life into other projects, as it will enable the creation of boards with USB interfaces, displays, and various peripherals. I’ll keep this post updated with the project’s progress.</p> <h2 id="my-pcb-manufacturing-process" data-svelte-h="svelte-g4sc4g"><a href="#my-pcb-manufacturing-process">My PCB Manufacturing Process</a></h2> <ol data-svelte-h="svelte-7e02j8"><li>Print the top and bottom layer on a laser-jet printer to the magazine paper.</li> <li>Put it on the copper plate, which is cleaned with steel wool.</li> <li>Run through the laminator 10 times.</li> <li>For the etching solution, in a ceramic or glass bowl:Half hydrogen-peroxide, half Hydrochloric acid.</li> <li>Clean again with acetone and steel wool.</li> <li>Mark the holes, then drill.</li></ol> <p data-svelte-h="svelte-1bzb17n">At this point, the PCB is ready to use, rest of the steps for adding solder mask layer</p> <ol start="7" data-svelte-h="svelte-mv6ubq"><li>Put solder resist film on the PCB and cut sides; the film will stick.</li> <li>Run through the laminator three times.</li> <li>Print the top solder mask layer to the transparency paper.</li> <li>Tape transparency paper on glass, put the PCB under the glass, align the top layer and solder mask.</li> <li>Expose it to UV light.</li> <li>Make sodium carbonate from sodium bicarbonate by heating or use sodium carbonete, can be find as washing soda.</li> <li>Proof the solder mask layer, dissolve pads.</li></ol> <h2 id="first-attempt-failed" data-svelte-h="svelte-1ymfcro"><a href="#first-attempt-failed">First Attempt: Failed</a></h2> <p data-svelte-h="svelte-184njcj">As today, April 4th, I have all the materials I need. I wanted to print a lithium battery charging PCB with USB Type-C.</p> <p data-svelte-h="svelte-16n5i9t">My initial attempt has failed catastrophically.</p> <p><img src="img/worst-pcb.webp" alt="attempt" class="rounded-lg my-2" loading="lazy" decoding="async"></p> <h4 id="what-went-wrong" data-svelte-h="svelte-kr9zzw"><a href="#what-went-wrong">What went wrong?</a></h4> <p data-svelte-h="svelte-1guwn3a">Couple of things went wrong. First one is the lamination process. Because I gave the copper plate with the magazine paper to the laminator in so many different angles at the start of the process, the paper on it became wrinkled. Also, I should have used a better magazine paper, because the one I used left its paint on the board.</p> <p data-svelte-h="svelte-mxkwso">Another issue was the etching solution. I made a big mistake by trusting my eyes. I didn’t measure the amounts of the chemicals I used. I just poured them into the bowl. Additionally, I shake the bowl too aggressively and the solution react too fast. It bubbled a lot and eat all the copper on the board as it is seen in the picture.</p> <h4 id="whats-next" data-svelte-h="svelte-mpcy8c"><a href="#whats-next">What’s next?</a></h4> <p data-svelte-h="svelte-13ybu6g">Next week, I will:</p> <ol data-svelte-h="svelte-1jsqjkc"><li>Find a better magazine paper.</li> <li>Make lamination process more controlled.</li> <li>Measure the amounts of the chemicals I use.</li> <li>Definitely use plastic gloves instead of plastic bag.</li></ol>]]>
    </content>
    <category term="PCB" scheme="https://behlulakbudak.com/?tags=PCB" />
  </entry>
</feed>